{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabh_hm/Documents/Courses/CS 6120/Project/DocuBOT/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "# Fundamental Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Langchain Libraries\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Miscellaneous\n",
    "import textwrap\n",
    "from IPython.display import Markdown\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Connection With Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Helper Function - Converts the input text into a Markdown blockquote format.\n",
    "def to_markdown(text):\n",
    "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _:True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Artificial intelligence (AI) is a broad field encompassing many techniques, but at its core, it's about creating systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and natural language understanding.  There's no single \"how it works,\" but rather a collection of approaches, broadly categorized as follows:\n",
       "> \n",
       "> **1. Machine Learning (ML):** This is the most prevalent approach currently.  Instead of explicitly programming a computer to perform a task, ML involves feeding it vast amounts of data and letting it learn patterns and relationships from that data.  The system adjusts its internal parameters (weights and biases) to improve its performance over time.  Key concepts include:\n",
       "> \n",
       "> * **Supervised Learning:** The algorithm is trained on a labeled dataset – meaning the data is already tagged with the correct answers.  Examples include image classification (images labeled with the objects they contain) and spam detection (emails labeled as spam or not spam).\n",
       "> * **Unsupervised Learning:** The algorithm is trained on an unlabeled dataset and tries to find patterns and structures within the data itself.  Examples include clustering (grouping similar data points together) and dimensionality reduction (reducing the number of variables while retaining important information).\n",
       "> * **Reinforcement Learning:** The algorithm learns through trial and error by interacting with an environment. It receives rewards for good actions and penalties for bad actions, learning to optimize its behavior to maximize rewards.  Examples include game playing (AlphaGo) and robotics.\n",
       "> \n",
       "> **2. Deep Learning (DL):** A subfield of ML that uses artificial neural networks with multiple layers (hence \"deep\").  These networks can learn highly complex patterns from data, making them particularly effective for tasks like image recognition, natural language processing, and speech recognition.  The \"deep\" refers to the many layers of interconnected nodes that process information in a hierarchical fashion, extracting increasingly abstract features from the raw data.\n",
       "> \n",
       "> **3. Expert Systems:** These systems mimic the decision-making ability of a human expert in a specific domain.  They use a knowledge base of rules and facts to infer conclusions and provide recommendations. While less common now than ML/DL, they are still used in niche applications.\n",
       "> \n",
       "> **4. Natural Language Processing (NLP):** Focuses on enabling computers to understand, interpret, and generate human language.  This includes tasks like machine translation, text summarization, sentiment analysis, and chatbots.  Often relies heavily on deep learning techniques.\n",
       "> \n",
       "> **5. Computer Vision:**  Enables computers to \"see\" and interpret images and videos.  This includes object detection, image segmentation, and facial recognition.  Deep learning has revolutionized this field.\n",
       "> \n",
       "> \n",
       "> **In essence, most AI systems work by:**\n",
       "> \n",
       "> 1. **Data Acquisition:** Gathering large amounts of relevant data.\n",
       "> 2. **Data Preprocessing:** Cleaning and preparing the data for the chosen algorithm.\n",
       "> 3. **Model Selection:** Choosing an appropriate algorithm (e.g., neural network, decision tree).\n",
       "> 4. **Training:** Feeding the data to the algorithm to learn patterns.\n",
       "> 5. **Evaluation:** Testing the model's performance on unseen data.\n",
       "> 6. **Deployment:** Using the trained model to make predictions or decisions on new data.\n",
       "> \n",
       "> \n",
       "> It's important to note that AI is still under development, and many current systems are narrow or weak AI – meaning they are designed for specific tasks and lack the general intelligence of humans.  The pursuit of artificial general intelligence (AGI), a system with human-level intelligence across various domains, remains a major goal of the field.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extracting text from PDFs\n",
    "text = \"\"\n",
    "file_path = '../dataset/Speech & Language Processing.pdf'\n",
    "with open(file_path, 'rb') as pdf_file:\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Converting extracted data into data chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "text_chunks = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Language is an inherently temporal phenomenon. Spoken language is a sequence of\\nacoustic events over time, and we comprehend and produce both spoken and written\\nlanguage as a sequential input stream. The temporal nature of language is reﬂected\\nin the metaphors we use; we talk of the ﬂow of conversations ,news feeds , and twitter\\nstreams , all of which emphasize that language is a sequence that unfolds in time.\\nThis temporal nature is reﬂected in some language processing algorithms. For\\nexample, the Viterbi algorithm we introduced for HMM part-of-speech tagging pro-\\nceeds through the input a word at a time, carrying forward information gleaned along\\nthe way. But other machine learning approaches, like those we’ve studied for senti-\\nment analysis or other text classiﬁcation tasks don’t have this temporal nature – they\\nassume simultaneous access to all aspects of their input.\\nThe feedforward networks of Chapter 7 also assumed simultaneous access, al-',\n",
       " 'The feedforward networks of Chapter 7 also assumed simultaneous access, al-\\nthough they also had a simple model for time. Recall that we applied feedforward\\nnetworks to language modeling by having them look only at a ﬁxed-size window\\nof words, and then sliding this window over the input, making independent predic-\\ntions along the way. This sliding-window approach is also used in the transformer\\narchitecture we will introduce in Chapter 9.\\nThis chapter introduces a deep learning architecture that offers an alternative\\nway of representing time: recurrent neural networks (RNNs), and their variants like\\nLSTMs. RNNs have a mechanism that deals directly with the sequential nature of\\nlanguage, allowing them to handle the temporal nature of language without the use of\\narbitrary ﬁxed-sized windows. The recurrent network offers a new way to represent\\nthe prior context, in its recurrent connections , allowing the model’s decision to',\n",
       " 'the prior context, in its recurrent connections , allowing the model’s decision to\\ndepend on information from hundreds of words in the past. We’ll see how to apply\\nthe model to the task of language modeling, to sequence modeling tasks like part-\\nof-speech tagging, and to text classiﬁcation tasks like sentiment analysis.\\n8.1 Recurrent Neural Networks\\nA recurrent neural network (RNN) is any network that contains a cycle within its\\nnetwork connections, meaning that the value of some unit is directly, or indirectly,\\ndependent on its own earlier outputs as an input. While powerful, such networks\\nare difﬁcult to reason about and to train. However, within the general class of recur-\\nrent networks there are constrained architectures that have proven to be extremely\\neffective when applied to language. In this section, we consider a class of recurrent\\nnetworks referred to as Elman Networks (Elman, 1990) or simple recurrent net-Elman\\nNetworks8.1 • R ECURRENT NEURAL NETWORKS 159',\n",
       " 'Networks8.1 • R ECURRENT NEURAL NETWORKS 159\\nworks . These networks are useful in their own right and serve as the basis for more\\ncomplex approaches like the Long Short-Term Memory (LSTM) networks discussed\\nlater in this chapter. In this chapter when we use the term RNN we’ll be referring to\\nthese simpler more constrained networks (although you will often see the term RNN\\nto mean any net with recurrent properties including LSTMs).\\nxthtyt\\nFigure 8.1 Simple recurrent neural network after Elman (1990). The hidden layer includes\\na recurrent connection as part of its input. That is, the activation value of the hidden layer\\ndepends on the current input as well as the activation value of the hidden layer from the\\nprevious time step.\\nFig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward net-\\nworks, an input vector representing the current input, xt, is multiplied by a weight\\nmatrix and then passed through a non-linear activation function to compute the val-',\n",
       " 'matrix and then passed through a non-linear activation function to compute the val-\\nues for a layer of hidden units. This hidden layer is then used to calculate a cor-\\nresponding output, yt. In a departure from our earlier window-based approach, se-\\nquences are processed by presenting one item at a time to the network. We’ll use\\nsubscripts to represent time, thus xtwill mean the input vector xat time t. The key\\ndifference from a feedforward network lies in the recurrent link shown in the ﬁgure\\nwith the dashed line. This link augments the input to the computation at the hidden\\nlayer with the value of the hidden layer from the preceding point in time .\\nThe hidden layer from the previous time step provides a form of memory, or\\ncontext, that encodes earlier processing and informs the decisions to be made at\\nlater points in time. Critically, this approach does not impose a ﬁxed-length limit\\non this prior context; the context embodied in the previous hidden layer can include']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text_chunks))\n",
    "text_chunks[500:505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding text chunks into vector format\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "vector_store.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/w1g1703d3d9b864tbbl9hnp80000gn/T/ipykernel_15454/2775522579.py:12: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(model, chain_type='stuff', prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "# 4. Building the conversation chain\n",
    "prompt_template = \"\"\"\n",
    "Answer the question as detailed as possible from the provided context and make sure to provide all the details.\n",
    "If the answer is not available in the provided context just say, \"Answer Not Available In Given Context\", DO NOT provide the wrong answer.\n",
    "Context : \\n{context}?\\n\n",
    "Question : \\n{question}\\n\n",
    "\n",
    "Answer : \n",
    "\"\"\"\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash', temperature=0.3)\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain = load_qa_chain(model, chain_type='stuff', prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/w1g1703d3d9b864tbbl9hnp80000gn/T/ipykernel_15454/1553092548.py:8: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the provided text, a Large Language Model (LLM) is a type of pretrained language model that learns knowledge about language and the world from vast amounts of text data (hundreds of billions of words, generally scraped from the web).  These models are built using transformers and are particularly powerful because they can be used to address many Natural Language Processing (NLP) tasks by framing them as word prediction problems.  The ability to incorporate the entirety of the earlier context and generated outputs at each step is key to their power.  LLMs exhibit remarkable performance on various NLP tasks, especially those involving text generation, such as summarization, machine translation, question answering, and chatbots.  The training data used for LLMs requires careful filtering for quality and balancing across domains, and transparency regarding this data is increasingly important due to fair use concerns and government regulations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Processing User Input\n",
    "user_question = \"What is Large Language Model\"\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "new_db = FAISS.load_local('faiss_index', embeddings, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(user_question)\n",
    "\n",
    "response = chain(\n",
    "    {'input_documents': docs, 'question': user_question},\n",
    "    return_only_outputs=True\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='sight of large language modeling is that many practical NLP tasks can be cast as\\nword prediction , and that a powerful-enough language model can solve them with\\na high degree of accuracy. For example, we can cast sentiment analysis as language\\nmodeling by giving a language model a context like:\\nThe sentiment of the sentence ``I like Jackie Chan\" is:\\nand comparing the following conditional probability of the words “positive” and the10.1 • L ARGE LANGUAGE MODELS WITH TRANSFORMERS 205\\nPreﬁx TextCompletion Text\\nEncoderTransformerBlocksSoftmax\\nlongall\\nandthanksforallthe\\nthe…UUUnencoder layerLanguage ModelingHeadlogits\\nSo\\nEi+\\nEi+\\nEi+\\nEi+\\nEi+\\nEi+\\nEi+…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP(positivejThe sentiment of the sentence ``I like Jackie Chan\" is: )'),\n",
       " Document(metadata={}, page_content='fair use, making it extremely important that language models include datasheets\\n(page 16) or model cards (page 74) giving full replicable information on the cor-\\npora used to train them. Open-source models can specify their exact training data.\\nRequirements that models are transparent in such ways is also in the process of being\\nincorporated into the regulations of various national governments.220 CHAPTER 10 • L ARGE LANGUAGE MODELS\\n10.7 Summary\\nThis chapter has introduced the large language model, and how it can be built out of\\nthe transformer. Here’s a summary of the main points that we covered:\\n• Many NLP tasks—such as question answering, summarization, sentiment,\\nand machine translation—can be cast as tasks of word prediction and hence\\naddressed with Large language models.\\n• Large language models are generally pretrained on large datasets of 100s of\\nbillions of words generally scraped from the web.\\n• These datasets need to be ﬁltered for quality and balanced for domains by'),\n",
       " Document(metadata={}, page_content='the LLM continue generating text token by token, conditioned on the prompt. The\\nfact that transformers have such long contexts (many thousands of tokens) makes\\nthem very powerful for conditional generation, because they can look back so far\\ninto the prompting text.\\nConsider the simple task of text completion, illustrated in Fig. 10.1. Here a\\nlanguage model is given a text preﬁx and is asked to generate a possible completion.\\nNote that as the generation process proceeds, the model has direct access to the\\npriming context as well as to all of its own subsequently generated outputs (at least\\nas much as ﬁts in the large context window). This ability to incorporate the entirety\\nof the earlier context and generated outputs at each time step is the key to the power\\nof large language models built from transformers.\\nSo why should we care about predicting upcoming words or tokens? The in-\\nsight of large language modeling is that many practical NLP tasks can be cast as'),\n",
       " Document(metadata={}, page_content='course, grounding from real-world interaction or other modalities can help build\\neven more powerful models, but even text alone is remarkably useful.\\nIn this chapter we formalize this idea of pretraining —learning knowledge about pretraining\\nlanguage and the world from vast amounts of text—and call the resulting pretrained\\nlanguage models large language models . Large language models exhibit remark-204 CHAPTER 10 • L ARGE LANGUAGE MODELS\\nable performance on all sorts of natural language tasks because of the knowledge\\nthey learn in pretraining, and they will play a role throughout the rest of this book.\\nThey have been especially transformative for tasks where we need to produce text,\\nlike summarization, machine translation, question answering, or chatbots.\\nWe’ll start by seeing how to apply the transformer of Chapter 9 to language\\nmodeling, in a setting often called causal or autoregressive language models, in')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Essential Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from giskard.rag import KnowledgeBase, generate_testset\n",
    "from giskard.rag import evaluate\n",
    "import giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up required configurations\n",
    "giskard.llm.set_llm_model(\"gemini/gemini-1.5-flash\")\n",
    "giskard.llm.set_embedding_model(\"gemini/text-embedding-004\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1 and substitutions are not allowed. (This is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>we saw it. We can do this by using dynamic pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>n t e n t i o ni n t e n t i o n\\ne t e n t i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>D[i;j]as the edit distance between X[1::i]andY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>We mentioned above two versions of Levenshtein...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "100  1 and substitutions are not allowed. (This is ...\n",
       "101  we saw it. We can do this by using dynamic pro...\n",
       "102  n t e n t i o ni n t e n t i o n\\ne t e n t i ...\n",
       "103  D[i;j]as the edit distance between X[1::i]andY...\n",
       "104  We mentioned above two versions of Levenshtein..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([chunk for chunk in text_chunks], columns=['text'])\n",
    "\n",
    "# Let's reduce the testing size for faster calculation\n",
    "df = df[100:250]\n",
    "knowledge_base = KnowledgeBase(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-09 03:09:46,128 pid:15454 MainThread giskard.rag  INFO     Finding topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-09 03:09:59,618 pid:15454 MainThread giskard.rag  INFO     Found 3 topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions: 100%|██████████| 5/5 [00:11<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "testset = generate_testset(\n",
    "    knowledge_base,\n",
    "    num_questions = 5,\n",
    "    agent_description = \"A chatbot that answers questions based on the content of a given PDF.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is the solution for unknown words that did not appear in the training documents?\n",
      "Reference answer: The solution for such unknown words is to ignore them—remove them from the test unknown word document and not include any probability for them at all.\n",
      "Reference context:\n",
      "Document 211: ulary at all because they did not occur in any training document in any class? The\n",
      "solution for such unknown words is to ignore them—remove them from the test unknown word\n",
      "document and not include any probability for them at all.\n",
      "Finally, some systems choose to completely ignore another class of words: stop\n",
      "words , very frequent words like theanda. This can be done by sorting the vocabu- stop words\n",
      "lary by frequency in the training set, and deﬁning the top 10–100 vocabulary entries\n",
      "as stop words, or alternatively by using one of the many predeﬁned stop word lists\n",
      "available online. Then each instance of these stop words is simply removed from\n",
      "both training and test documents as if it had never occurred. In most text classiﬁca-\n",
      "tion applications, however, using a stop word list doesn’t improve performance, and\n",
      "so it is more common to make use of the entire vocabulary and not use a stop word\n",
      "list.\n",
      "Fig. 4.2 shows the ﬁnal algorithm.\n",
      "4.3 Worked example\n",
      "******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testset.save(\"../dataset/test_set.jsonl\")\n",
    "\n",
    "testset_df = testset.to_pandas()\n",
    "for index, row in enumerate(testset_df.head(1).iterrows()):\n",
    "    print(f\"Question {index + 1}: {row[1]['question']}\")\n",
    "    print(f\"Reference answer: {row[1]['reference_answer']}\")\n",
    "    print(\"Reference context:\")\n",
    "    print(row[1]['reference_context'])\n",
    "    print(\"******************\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context and make sure to provide all the details.\n",
    "    If the answer is not available in the provided context just say, \"Answer Not Available In Given Context\", DO NOT provide the wrong answer.\n",
    "    Context : \\n{context}?\\n\n",
    "    Question : \\n{question}\\n\n",
    "\n",
    "    Answer : \n",
    "    \"\"\"\n",
    "    model = ChatGoogleGenerativeAI(model='gemini-1.5-pro', temperature=0.3)\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain = load_qa_chain(model, chain_type='stuff', prompt=prompt)\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_fn(question, history=None):\n",
    "    # If history is not needed, we can ignore it. However, if you want to implement conversation memory, this can be added.\n",
    "    \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "    new_db = FAISS.load_local('faiss_index', embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    docs = new_db.similarity_search(question)\n",
    "    chain = get_conversational_chain()\n",
    "    \n",
    "    response = chain(\n",
    "        {'input_documents': docs, 'question': question},\n",
    "        return_only_outputs=True\n",
    "    )\n",
    "    \n",
    "    answer = response['output_text']\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent:  80%|████████  | 4/5 [00:12<00:02,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-09 03:15:47,126 pid:15454 MainThread langchain_google_genai.chat_models WARNING  Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent: 100%|██████████| 5/5 [00:17<00:00,  3.52s/it]\n",
      "CorrectnessMetric evaluation: 100%|██████████| 5/5 [00:03<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating RAG evaluation report\n",
    "report = evaluate(answer_fn, testset=testset, knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"c8516ef9-11cb-47ee-a92f-5be78ebf3f78\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"c8516ef9-11cb-47ee-a92f-5be78ebf3f78\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"c8516ef9-11cb-47ee-a92f-5be78ebf3f78\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "\n",
       "<style>\n",
       "    body{\n",
       "  background: #18181B;\n",
       "}\n",
       "\n",
       ".main{\n",
       "  font-family: \"Noto Sans\", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\";\n",
       "  color: #FDFDFD;\n",
       "}\n",
       "\n",
       "h1 {\n",
       "  font-size: 2.5rem;\n",
       "  color: white;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "  font-size: 1.5rem;\n",
       "  background: #0c087c;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border: 1px solid #6b7280;}\n",
       "\n",
       ".extended-title{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#gsk-overview{\n",
       "  display:flex;\n",
       "}\n",
       "\n",
       "h4 {\n",
       "  font-size: 1rem;\n",
       "  background: #27272A;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "  font-size: 1.5rem;\n",
       "  margin-top: 3px;\n",
       "  color:#000000;\n",
       "}\n",
       "\n",
       ".header{\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       ".header > * {\n",
       "  margin-inline: 20px;\n",
       "}\n",
       "\n",
       ".flex-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding:10px;\n",
       "  border: 1px solid #27272A;\n",
       "}\n",
       "\n",
       ".flex-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "progress[value] {\n",
       "  --background: #6D6D6D;\n",
       "  -webkit-appearance: none;\n",
       "  -moz-appearance: none;\n",
       "  appearance: none;\n",
       "  border: none;\n",
       "  height: 4px;\n",
       "  margin: 0 10px;\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-value {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "progress[value]::-moz-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "label {\n",
       "  font-size: 20px;\n",
       "  font-weight: bold;\n",
       "  display: block;\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".tab {\n",
       "  overflow: hidden;\n",
       "  border: 1px solid #27272A;\n",
       "  background-color: #27272A;\n",
       "}\n",
       "\n",
       ".tab button {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size:1.2rem;\n",
       "}\n",
       "\n",
       ".tab div {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size: 1.2rem;\n",
       "}\n",
       "\n",
       ".tab button:hover {\n",
       "  background-color: #18181B;\n",
       "}\n",
       "\n",
       ".tab-title{\n",
       "  font-size: 1.5rem;\n",
       "  font-weight: bold;\n",
       "  margin-bottom:-5px;\n",
       "}\n",
       "\n",
       ".tab button.active {\n",
       "  background-color: #18181B;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  border-bottom: 1px solid #18181B;\n",
       "  border-left: 1px solid #6b7280;\n",
       "  border-right: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       ".tabcontent {\n",
       "  display: none;\n",
       "  padding: 6px 12px;\n",
       "  background: #18181B;\n",
       "  border: 1px solid #27272A;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  margin-top: -2px;\n",
       "}\n",
       "\n",
       "#gsk-advice {\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "}\n",
       "\n",
       "#gsk-metrics{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#recommendation {\n",
       "  margin-top: 20px;\n",
       "  padding: 20px;\n",
       "  border-radius: 10px;\n",
       "  background-color: #e1ce86;\n",
       "  color: #27272A;\n",
       "  width:95%;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "\n",
       ".separator {\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".separator-border {\n",
       "  margin: 20px 0;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "#gsk-rag{\n",
       "  margin: 32px 28px;\n",
       "  padding: 12px 24px;\n",
       "  background-color: #111516;\n",
       "}\n",
       "\n",
       ".section-container {\n",
       "  margin-bottom: 32px;\n",
       "}\n",
       "\n",
       "  .components-container {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    align-items: flex-start;\n",
       "    gap: 0 32px;\n",
       "  }\n",
       "\n",
       "    .component-card {\n",
       "      background-color: #14191B;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .component-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #B1B1B1;\n",
       "      padding-bottom: 8px;\n",
       "    }\n",
       "\n",
       "    .component-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "    }\n",
       "      \n",
       "      .text-green {\n",
       "        color: #04B543;\n",
       "      }\n",
       "      \n",
       "      .text-orange {\n",
       "        color: #E76E0F;\n",
       "      }\n",
       "      \n",
       "      .text-red {\n",
       "        color: #EA3829;\n",
       "      }\n",
       "\n",
       "      .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        width: 120px;\n",
       "        background-color: #464646;\n",
       "        color: #E6E6E6;\n",
       "        text-align: center;\n",
       "        border-radius: 6px;\n",
       "        position: absolute;\n",
       "        z-index: 1;\n",
       "        top: 150%;\n",
       "        left: 50%;\n",
       "        margin-left: -60px;\n",
       "        font-size: 12px;\n",
       "        padding: 12px;\n",
       "      }\n",
       "      \n",
       "      .tooltip .tooltiptext::after {\n",
       "        content: \"\";\n",
       "        position: absolute;\n",
       "        bottom: 100%;\n",
       "        left: 50%;\n",
       "        margin-left: -5px;\n",
       "        border-width: 5px;\n",
       "        border-style: solid;\n",
       "        border-color: transparent transparent black transparent;\n",
       "      }\n",
       "      \n",
       "      .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "      }\n",
       "\n",
       "    .overall-card {\n",
       "      background-color: #026836;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      justify-content: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "  \n",
       "    .overall-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #E6E6E6;\n",
       "      padding: 14px 0 8px 0;\n",
       "      text-transform: uppercase;\n",
       "     }\n",
       "  \n",
       "    .overall-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "      color: #E6E6E6;\n",
       "    }\n",
       "\n",
       ".section-title {\n",
       "  font-size: 12px;\n",
       "  color: #B1B1B1;\n",
       "  margin-bottom: 20px;\n",
       "  text-align: left;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".section-content {\n",
       "  color: #E6E6E6;\n",
       "  font-size: 20px;\n",
       "  line-height: 1.5;\n",
       "}\n",
       "\n",
       ".section-card {\n",
       "  background-color: #14191B;\n",
       "  border-radius: 16px;\n",
       "  padding: 28px 32px 32px 32px;\n",
       "  display: flex;\n",
       "  flex-flow: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       ".correctness-indicator{\n",
       "  padding: 20px;\n",
       "  border-radius: 50px;\n",
       "  font-size: 16pt;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".metric-title{\n",
       "  margin: -2px;\n",
       "  border-bottom: none;\n",
       "}\n",
       "\n",
       ".hist-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding: 10px;\n",
       "  justify-content: space-around;\n",
       "  width: 85%;\n",
       "}\n",
       "\n",
       ".hist-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "  padding-left: 1%;\n",
       "  padding-right: 1%;\n",
       "}\n",
       "\n",
       ".tab-row{\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "#component-table{\n",
       "  width:50%;\n",
       "  margin-top: 10px;\n",
       "}\n",
       "\n",
       ".green{\n",
       "  background-color: #0a980a;\n",
       "}\n",
       "\n",
       ".orange {\n",
       "  background-color: #e5b62a;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  background-color: #ba0e0e;\n",
       "}\n",
       "\n",
       ".progress-green {\n",
       "  --color: #04B543;\n",
       "}\n",
       "\n",
       ".progress-orange {\n",
       "  --color: #E76E0F;\n",
       "}\n",
       "\n",
       ".progress-red {\n",
       "  --color: #EA3829;\n",
       "}\n",
       "\n",
       ".corr-plot{\n",
       "  flex: 1;\n",
       "  padding-left: 2%;\n",
       "}\n",
       "\n",
       ".tooltip-text {\n",
       "  position: absolute;\n",
       "  display: none;\n",
       "  visibility: hidden;\n",
       "  z-index: 1;\n",
       "  top: 100%;\n",
       "  left: 0%;\n",
       "  width: 100%;\n",
       "  color: white;\n",
       "  font-size: 12px;\n",
       "  background-color: #2d3d4c;\n",
       "  border-radius: 10px;\n",
       "  padding: 10px 15px 10px 15px;\n",
       "}\n",
       "\n",
       "#fade {\n",
       "  opacity: 1;\n",
       "  transition: opacity 0.5s;\n",
       "}\n",
       "\n",
       "#delay {\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s;\n",
       "  transition-delay: 1s;\n",
       "}\n",
       "\n",
       "td {\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "tr:hover .tooltip-text {\n",
       "  display: block;\n",
       "  visibility: visible;\n",
       "}\n",
       "\n",
       ".tr:hover #fade {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".tr:hover #delay {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".callout {\n",
       "  padding: 0.5rem 1rem 0.5rem 3rem;\n",
       "  background: #D9EDF9;\n",
       "  border: 3px solid #0088D1;\n",
       "  color: #272eb5;\n",
       "  position: relative;\n",
       "  max-width: 40rem;\n",
       "  border-radius: 10px;\n",
       "  margin-top: 10%;\n",
       "  font-size: 11pt;\n",
       "}\n",
       "\n",
       ".callout-icon {\n",
       "  content: \"\";\n",
       "\n",
       "  /* SVG via a data URI! */\n",
       "  background-size: cover;\n",
       "  width: 1.5rem;\n",
       "  height: 1.5rem;\n",
       "  display: block;\n",
       "  position: absolute;\n",
       "  left: 0.9rem;\n",
       "  top: 1.1rem;\n",
       "}\n",
       "\n",
       ".callout-icon svg{\n",
       "  fill: #016ca7;\n",
       "}\n",
       ".callout p+p {\n",
       "  margin-top: 1em;\n",
       "}\n",
       "\n",
       ".callout a {\n",
       "  color: #272eb5;\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       "#gsk-logo {\n",
       "  padding-top: 10px;\n",
       "}\n",
       "</style>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\" integrity=\"sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf\" crossorigin=\"anonymous\"></script>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\" integrity=\"sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class=\"main\">\n",
       "    <div id=\"gsk-rag\" class=\"dark:text-white dark:bg-zinc-800 rounded border border-gray-500\">\n",
       "        <div class=\"header border-b border-b-gray-500\">\n",
       "            \n",
       "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"60\" height=\"30\" viewBox=\"0 0 30 15\" fill=\"none\" id=\"gsk-logo\">\n",
       "                    <path fill=\"#fff\" fill-rule=\"evenodd\"\n",
       "                        d=\"M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z\"\n",
       "                        clip-rule=\"evenodd\" />\n",
       "                </svg>\n",
       "            <h1>RAG Evaluation Toolkit</h1>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"components-container\">\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">GENERATOR</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        80.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Generator is the LLM inside the RAG to generate the answers.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=80.0 class=\" progress-green \">80.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">RETRIEVER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        50.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=50.0 class=\" progress-orange \">50.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">REWRITER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        50.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=50.0 class=\" progress-orange \">50.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">ROUTING</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Router filters the query of the user based on his intentions (intentions detection).</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">KNOWLEDGE_BASE</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"overall-card\">\n",
       "                    <div class=\"overall-title\">Overall Correctness Score</div>\n",
       "                    <div class=\"overall-value\">80%</div>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">RECOMMENDATION</div>\n",
       "                <span class=\"section-content\">The low score on &#34;distracting element&#34; questions points to a significant Retriever or Rewriter issue; focus on improving these components to handle irrelevant information in user queries, which should also improve the overall Language Modeling topic score.\n",
       "</span>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">CORRECTNESS BY TOPIC</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"cc31406b-f25f-478c-ae44-694a3f973d2d\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1189\",\"attributes\":{\"height\":350,\"width_policy\":\"max\",\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1191\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1198\",\"attributes\":{\"factors\":[\"Language Modeling\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1199\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1200\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1196\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1218\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1186\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1187\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1188\"},\"data\":{\"type\":\"map\",\"entries\":[[\"correctness\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAAVEA=\"},\"shape\":[1],\"dtype\":\"float64\",\"order\":\"little\"}],[\"metadata_values\",[\"Language Modeling\"]],[\"colors\",[\"#a50026\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1219\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1220\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1215\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1216\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1217\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1227\",\"attributes\":{\"data_source\":{\"id\":\"p1186\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1228\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1229\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1224\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1225\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1226\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1237\",\"attributes\":{\"visible\":false,\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1231\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1232\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1233\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[0]],[\"y\",[0]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1238\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1239\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1234\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1235\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.1,\"line_width\":2,\"line_dash\":[6]}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1236\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.2,\"line_width\":2,\"line_dash\":[6]}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1197\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1211\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"topic\",\"@metadata_values\"],[\"Correctness\",\"@correctness{0.00}\"]]}}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1206\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1207\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1208\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1209\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1201\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1202\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1203\"},\"axis_label\":\"Correctness (%)\",\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1204\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1205\",\"attributes\":{\"axis\":{\"id\":\"p1201\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1210\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1206\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Span\",\"id\":\"p1230\",\"attributes\":{\"location\":80.0,\"dimension\":\"height\",\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1240\",\"attributes\":{\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1241\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Correctness on the entire Testset\"},\"renderers\":[{\"id\":\"p1237\"}]}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"cc31406b-f25f-478c-ae44-694a3f973d2d\",\"roots\":{\"p1189\":\"decff3a2-bf0c-4782-9f25-b934977f5311\"},\"root_ids\":[\"p1189\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"decff3a2-bf0c-4782-9f25-b934977f5311\" data-root-id=\"p1189\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">KNOWLEDGE BASE OVERVIEW</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"651aa3fc-9bb5-422c-a02f-5f6851b6f043\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Tabs\",\"id\":\"p1120\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"tabs\":[{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1050\",\"attributes\":{\"title\":\"Topic exploration\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1013\",\"attributes\":{\"start\":5.435843300819397,\"end\":17.402650928497316}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1014\",\"attributes\":{\"start\":-11.130775928497314,\"end\":0.03082120418548584}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1016\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1011\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1044\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"W3dEQeRxPUF5TEtBvDZHQT7RUEFsPkpBWblOQVyyTkHe+URBzJ9EQbjXSUGhW0FB7uE3QQwePUHMqjVBj1Y0QRAMOkFWrBhBKEYUQTPKE0Gkzw1B8KoMQRhfC0HqnwJBwoABQZLkA0EtvAhBe5oTQU1rAkGXHwxBBD8kQUurHEGnCRxBGvISQfqy/kCtcA5BhmYHQas9AEE2ud9Ap4PhQORV5kD5Uu1A/p7TQKid0kCxUuFAQGnZQAjJ0UB3YepAZsn0QMsA2ECjXOtAJgb3QIBJ8UBM0wNBaTUNQRs+CUGM8QdB0DsOQSRQB0FBAA1B9WAMQYdODEHnyQNB/JEQQV1THEGMuhpB+0knQUx+JUFi7CJBOoslQcXWFEFuRBtBwLYVQRi9DUG0fRRBZZrfQDTw3kDRV+dAC//bQO2a80AfLPxAGI/oQEXZ4kDv69lAfAcCQdXZG0FMYRxB7S0fQZcrIUEHuRdBt0UfQf9VGkFWKCJBXiQCQQyuCEGAXQ1BOvYdQYiTDUFuKAJBKUH7QGTYEkE35A9By78KQfcwCEF5gxRBOsgIQe9KE0Hm4glBj54RQS1iC0E1YBZBK88EQQVACEEBlApBBcsDQZUtFEGozhFBu3oQQYFJGkEoKhZBVq4ZQXrTH0GPlR5BCU0bQRwHDkEW1wFBHH0DQT77AEEtXvJAUkj7QIPj6kCDd/VAwCwBQYqQ8UBLkOdAhlfaQI3d6UDyud1Aa6jZQE1qw0AgC8hAhS/HQKGwyEAd+8ZAq2L1QGmGxkBceNNA5lzSQBmNuUC6m9tA\"},\"shape\":[150],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"xyDRwEV1wMDoHNHA26XGwCKyyMAxMrTAc/a4wEmptMC2hr7AdNDMwKgHvcB4tLPAYRu7wH+fqcDyYcbAx23XwNTf2sCsyfzATzT0wISe/cATSPHADsELwaxh7MCeO+zAXJMEwYHOBcFfkQnBvP8HwRIKEMECVBTBFSkNwZ/+EsECQgzBfogOwVZj+MAplADBio3+wGXwtcBVH7bAyZGfwDwrqcD2GqXAXAi4wDNfqMAmydDAeNTKwCLD1MC+2drAjqfXwFZDyMACx8bAJMzNwKibw8BsgcvAgeruwMKa9cCR4ADB2egEwScn3MBZS97AgBOswNurwcB+MrHAvQC4wKE3ucD/mNXAnFwGwfU4AMHSmhHBFxPqwDAR0cCem+LAPpvjwDNDysDb8MDAXA7hwM1888AIVgLBV4QAwfWE68AwkALBbt3ywLNQ8sCwWOXAKWvSwCmP0MBtyeXAPmPLwPypw8CU3f7AaDMOwfdABcFVKfnAJbt+wPecNMC92A7AEv1lwJiEa8CzlWPAAGeCwMIFf8BObV/ARRc7wP+KiMBPDo/A/yaawC20f8BbknnAbDSFwC6qksA5nqDAcWCUwDa0VcAdiojACYxgwCyrOcBW1QzASwsmwMyEQMD0IzTAhaBUwLP6eMDq/4nAA2iOwH/WUsAC0B/ACXr8v1MEDMCk7gnAQSvqvyI/EMCkOzTAKb8uwKn6J8BEL2TAEYeOwIJdhcC7UHXAF5Y/wMeYYcD+TmjA30ODwOg+bMBrZ1HAhH9TwDxpK8B1i0jATUc0wGUCRsCC7yrA\"},\"shape\":[150],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"String Algorithms\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\"]],[\"id\",[100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249]],[\"content\",[\"1 and substitutions are not allowed. (This is equivalent to allowing substitution, but\\\\ngiving each substitution a cost of 2 since any substitution can be represented by one\\\\ninsertion and one deletion). Using this version, the Levenshtein distance between\\\\nintention andexecution is 8.\\\\n2.8.1 The Minimum Edit Distance Algorithm\\\\nHow do we \\\\ufb01nd the minimum edit distance? We can think of this as a search task, in\\\\nwhich we are searching for the shortest path\\\\u2014a sequence of edits\\\\u2014from one string\\\\nto another...\",\"we saw it. We can do this by using dynamic programming . Dynamic programmingdynamic\\\\nprogramming\\\\nis the name for a class of algorithms, \\\\ufb01rst introduced by Bellman (1957), that apply\\\\na table-driven method to solve problems by combining solutions to subproblems.\\\\nSome of the most commonly used algorithms in natural language processing make\\\\nuse of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the\\\\nCKY algorithm for parsing (Chapter 18).\\\\nThe intuition of a dynamic programming prob...\",\"n t e n t i o ni n t e n t i o n\\\\ne t e n t i o n\\\\ne x e n t i o n\\\\ne x e n u t i o n\\\\ne x e c u t i o ndelete i\\\\nsubstitute n by e\\\\nsubstitute t by x\\\\ninsert u\\\\nsubstitute n by c\\\\nFigure 2.16 Path from intention toexecution .\\\\noperation list, then the optimal sequence must also include the optimal path from\\\\nintention toexention . Why? If there were a shorter path from intention toexention ,\\\\nthen we could use it instead, resulting in a shorter overall path, and the optimal\\\\nsequence wouldn\\\\u2019t be optimal, th...\",\"D[i;j]as the edit distance between X[1::i]andY[1::j], i.e., the \\\\ufb01rst icharacters of X\\\\nand the \\\\ufb01rst jcharacters of Y. The edit distance between XandYis thus D[n;m].\\\\nWe\\\\u2019ll use dynamic programming to compute D[n;m]bottom up, combining so-\\\\nlutions to subproblems. In the base case, with a source substring of length ibut an\\\\nempty target string, going from icharacters to 0 requires ideletes. With a target\\\\nsubstring of length jbut an empty source going from 0 characters to jcharacters\\\\nrequires jinserts....\",\"We mentioned above two versions of Levenshtein distance, one in which substitu-\\\\ntions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion\\\\nplus a deletion). Let\\\\u2019s here use that second version of Levenshtein distance in which\\\\nthe insertions and deletions each have a cost of 1 (ins-cost( \\\\u0001) = del-cost(\\\\u0001) = 1), and\\\\nsubstitutions have a cost of 2 (except substitution of identical letters has zero cost).\\\\nUnder this version of Levenshtein, the computation for D[i;j]become...\",\"Alignment Knowing the minimum edit distance is useful for algorithms like \\\\ufb01nd-\\\\ning potential spelling error corrections. But the edit distance algorithm is important28 CHAPTER 2 \\\\u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\\\\nfunction MIN-EDIT-DISTANCE (source ,target )returns min-distance\\\\nn LENGTH (source )\\\\nm LENGTH (target )\\\\nCreate a distance matrix D[n+1,m+1]\\\\n#Initialization: the zeroth row and column is the distance from the empty string\\\\nD[0,0] = 0\\\\nforeach row ifrom 1tondo\\\\nD[i,0] D[i-1,...\",\"programming algorithms. The various costs can either be \\\\ufb01xed (e.g., 8x;ins-cost (x) =1)\\\\nor can be speci\\\\ufb01c to the letter (to model the fact that some letters are more likely to be in-\\\\nserted than others). We assume that there is no cost for substituting a letter for itself (i.e.,\\\\nsub-cost (x;x) =0).\\\\nSrcnTar # e x e c u t i o n\\\\n# 0 1 2 3 4 5 6 7 8 9\\\\ni 1 2 3 4 5 6 7 6 7 8\\\\nn 2 3 4 5 6 7 8 7 8 7\\\\nt 3 4 5 6 7 8 7 8 9 8\\\\ne 4 3 4 5 6 7 8 9 10 9\\\\nn 5 4 5 6 7 8 9 10 11 10\\\\nt 6 5 6 7 8 9 8 9 10 11\\\\ni 7 6 7 8 9 ...\",\"ment between two strings. Aligning two strings is useful throughout speech and\\\\nlanguage processing. In speech recognition, minimum edit distance alignment is\\\\nused to compute the word error rate (Chapter 16). Alignment plays a role in ma-\\\\nchine translation, in which sentences in a parallel corpus (a corpus with a text in two\\\\nlanguages) need to be matched to each other.\\\\nTo extend the edit distance algorithm to produce an alignment, we can start by\\\\nvisualizing an alignment as a path through the edi...\",\"Figure 2.19 also shows the intuition of how to compute this alignment path. The\\\\ncomputation proceeds in two steps. In the \\\\ufb01rst step, we augment the minimum edit\\\\ndistance algorithm to store backpointers in each cell. The backpointer from a cell\\\\npoints to the previous cell (or cells) that we came from in entering the current cell.\\\\nWe\\\\u2019ve shown a schematic of these backpointers in Fig. 2.19. Some cells have mul-\\\\ntiple backpointers because the minimum extension could have come from multiple\\\\nprevious ...\",\"output an alignment.\\\\n# e x e c u t i o n\\\\n# 0 1 2 3 4 5 6 7 8 9\\\\ni\\\\\"1- \\\\\" 2- \\\\\" 3- \\\\\" 4- \\\\\" 5- \\\\\" 6- \\\\\" 7-6 7 8\\\\nn\\\\\"2- \\\\\" 3- \\\\\" 4- \\\\\" 5- \\\\\" 6- \\\\\" 7- \\\\\" 8\\\\\"7- \\\\\" 8-7\\\\nt\\\\\"3- \\\\\" 4- \\\\\" 5- \\\\\" 6- \\\\\" 7- \\\\\" 8-7 \\\\\"8- \\\\\" 9\\\\\"8\\\\ne\\\\\"4-3 4- 5 6 7 \\\\\"8- \\\\\" 9- \\\\\" 10\\\\\"9\\\\nn\\\\\"5\\\\\"4- \\\\\" 5- \\\\\" 6- \\\\\" 7- \\\\\" 8- \\\\\" 9- \\\\\" 10- \\\\\" 11-\\\\\"10\\\\nt\\\\\"6\\\\\"5- \\\\\" 6- \\\\\" 7- \\\\\" 8- \\\\\" 9-8 9 10 \\\\\"11\\\\ni\\\\\"7\\\\\"6- \\\\\" 7- \\\\\" 8- \\\\\" 9- \\\\\" 10\\\\\"9-8 9 10\\\\no\\\\\"8\\\\\"7- \\\\\" 8- \\\\\" 9- \\\\\" 10- \\\\\" 11\\\\\"10\\\\\"9-8 9\\\\nn\\\\\"9\\\\\"8- \\\\\" 9- \\\\\" 10- \\\\\" 11- \\\\\" 12\\\\\"11\\\\\"10\\\\\"9-8\\\\nFigure 2.19 When entering a value in each cell, we mark which of the three neighboring...\",\"insertions or deletions, 2 for substitutions. Diagram design after Gus\\\\ufb01eld (1997).\\\\nWhile we worked our example with simple Levenshtein distance, the algorithm\\\\nin Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for\\\\nexample, substitutions are more likely to happen between letters that are next to\\\\neach other on the keyboard. The Viterbi algorithm is a probabilistic extension of\\\\nminimum edit distance. Instead of computing the \\\\u201cminimum edit distance\\\\u201d between\\\\ntwo strings...\",\"We also introduced the important minimum edit distance algorithm for comparing\\\\nstrings. Here\\\\u2019s a summary of the main points we covered about these ideas:30 CHAPTER 2 \\\\u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\\\\n\\\\u2022 The regular expression language is a powerful tool for pattern-matching.\\\\n\\\\u2022 Basic operations in regular expressions include concatenation of symbols,\\\\ndisjunction of symbols ( [],|),counters (*,+, and{n,m} ),anchors (^,$)\\\\nand precedence operators ( (,)).\\\\n\\\\u2022Word tokenization and nor...\",\"computed by dynamic programming , which also results in an alignment of\\\\nthe two strings.\\\\nBibliographical and Historical Notes\\\\nKleene 1951; 1956 \\\\ufb01rst de\\\\ufb01ned regular expressions and the \\\\ufb01nite automaton, based\\\\non the McCulloch-Pitts neuron. Ken Thompson was one of the \\\\ufb01rst to build regular\\\\nexpressions compilers into editors for text searching (Thompson, 1968). His edi-\\\\ntoredincluded a command \\\\u201cg/regular expression/p\\\\u201d, or Global Regular Expression\\\\nPrint, which later became the Unix grep utility.\\\\nTex...\",\"Tokenizer ( https://nlp.stanford.edu/software/tokenizer.shtml ) or spe-\\\\ncialized tokenizers for Twitter (O\\\\u2019Connor et al., 2010), or for sentiment ( http:\\\\n//sentiment.christopherpotts.net/tokenizing.html ). See Palmer (2012)\\\\nfor a survey of text preprocessing. NLTK is an essential tool that offers both useful\\\\nPython libraries ( https://www.nltk.org ) and textbook descriptions (Bird et al.,\\\\n2009) of many algorithms including text normalization and corpus interfaces.\\\\nFor more on Herdan\\\\u2019s law and He...\",\"the term dynamic programming :\\\\n\\\\u201c...The 1950s were not good years for mathematical research. [the]\\\\nSecretary of Defense ...had a pathological fear and hatred of the word,\\\\nresearch... I decided therefore to use the word, \\\\u201cprogramming\\\\u201d. I\\\\nwanted to get across the idea that this was dynamic, this was multi-\\\\nstage... I thought, let\\\\u2019s ... take a word that has an absolutely precise\\\\nmeaning, namely dynamic... it\\\\u2019s impossible to use the word, dynamic,\\\\nin a pejorative sense. Try thinking of some combinati...\",\"ately preceded by and immediately followed by a b;\\\\n2.2 Write regular expressions for the following languages. By \\\\u201cword\\\\u201d, we mean\\\\nan alphabetic string separated from other words by whitespace, any relevant\\\\npunctuation, line breaks, and so forth.\\\\n1. the set of all strings with two consecutive repeated words (e.g., \\\\u201cHum-\\\\nbert Humbert\\\\u201d and \\\\u201cthe the\\\\u201d but not \\\\u201cthe bug\\\\u201d or \\\\u201cthe big bug\\\\u201d);\\\\n2. all strings that start at the beginning of the line with an integer and that\\\\nend at the end of the line with a w...\",\"chologist, although keep in mind that you would need a domain in which your\\\\nprogram can legitimately engage in a lot of simple repetition.\\\\n2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution\\\\ncost 1) of \\\\u201cleda\\\\u201d to \\\\u201cdeal\\\\u201d. Show your work (using the edit distance grid).\\\\n2.5 Figure out whether drive is closer to brief or to divers and what the edit dis-\\\\ntance is to each. You may use any version of distance that you like.\\\\n2.6 Now implement a minimum edit distance algor...\",\"Random sentence generated from a Jane Austen trigram model\\\\nPredicting is dif\\\\ufb01cult\\\\u2014especially about the future, as the old quip goes. But how\\\\nabout predicting something that seems much easier, like the next word someone is\\\\ngoing to say? What word, for example, is likely to follow\\\\nThe water of Walden Pond is so beautifully ...\\\\nYou might conclude that a likely word is blue , orgreen , orclear , but probably not\\\\nrefrigerator northis . In this chapter we formalize this intuition by introducing\\\\nlangua...\",\"on guys all I of notice sidewalk three a sudden standing the\\\\nWhy would we want to predict upcoming words, or know the probability of a sen-\\\\ntence? One reason is for generation: choosing contextually better words. For ex-\\\\nample we can correct grammar or spelling errors like Their are two midterms ,\\\\nin whichThere was mistyped as Their , orEverything has improve , in which\\\\nimprove should have been improved . The phrase There are is more probable\\\\nthanTheir are , andhas improved thanhas improve , so ...\",\"speak or sign but can instead use eye gaze or other movements to select words from\\\\na menu. Word prediction can be used to suggest likely words for the menu.\\\\nWord prediction is also central to NLP for another reason: large language mod-\\\\nelsare built just by training them to predict words!! As we\\\\u2019ll see in chapters 7-9,\\\\nlarge language models learn an enormous amount about language solely from being\\\\ntrained to predict upcoming words from neighboring words.\\\\nIn this chapter we introduce the simplest ...\",\"gram\\\\u2019 to mean a probabilistic model that can estimate the probability of a word given\\\\nthe n-1 previous words, and thereby also to assign probabilities to entire sequences.\\\\nIn later chapters we will introduce the much more powerful neural large lan-\\\\nguage models , based on the transformer architecture of Chapter 9. But because\\\\nn-grams have a remarkably simple and clear formalization, we use them to intro-\\\\nduce some major concepts of large language modeling, including training and test\\\\nsets,perple...\",\"One way to estimate this probability is directly from relative frequency counts: take a\\\\nvery large corpus, count the number of times we see The water of Walden Pond\\\\nis so beautifully , and count the number of times this is followed by blue . This\\\\nwould be answering the question \\\\u201cOut of the times we saw the history h, how many\\\\ntimes was it followed by the word w\\\\u201d, as follows:\\\\nP(bluejThe water of Walden Pond is so beautifully ) =\\\\nC(The water of Walden Pond is so beautifully blue )\\\\nC(The water of W...\",\"for such large objects as entire sentences. For this reason, we\\\\u2019ll need more clever\\\\nways to estimate the probability of a word wgiven a history h, or the probability of\\\\nan entire word sequence W.\\\\nLet\\\\u2019s start with some notation. First, throughout this chapter we\\\\u2019ll continue to\\\\nrefer to words , although in practice we usually compute language models over to-\\\\nkens like the BPE tokens of page 21. To represent the probability of a particular\\\\nrandom variable Xitaking on the value \\\\u201cthe\\\\u201d, or P(Xi=\\\\u201cthe\\\\u201d)...\",\"quence having a particular value P(X1=w1;X2=w2;X3=w3;:::;Xn=wn)we\\\\u2019ll\\\\nuseP(w1;w2;:::;wn).\\\\nNow, how can we compute probabilities of entire sequences like P(w1;w2;:::;wn)?\\\\nOne thing we can do is decompose this probability using the chain rule of proba-\\\\nbility :\\\\nP(X1:::Xn) = P(X1)P(X2jX1)P(X3jX1:2):::P(XnjX1:n\\\\u00001)\\\\n=nY\\\\nk=1P(XkjX1:k\\\\u00001) (3.3)34 CHAPTER 3 \\\\u2022 N- GRAM LANGUAGE MODELS\\\\nApplying the chain rule to words, we get\\\\nP(w1:n) = P(w1)P(w2jw1)P(w3jw1:2):::P(wnjw1:n\\\\u00001)\\\\n=nY\\\\nk=1P(wkjw1:k\\\\u00001) (3.4)\\\\nThe chain...\",\"exact probability of a word given a long sequence of preceding words, P(wnjw1:n\\\\u00001).\\\\nAs we said above, we can\\\\u2019t just estimate by counting the number of times every word\\\\noccurs following every long string in some corpus, because language is creative and\\\\nany particular context might have never occurred before!\\\\n3.1.1 The Markov assumption\\\\nThe intuition of the n-gram model is that instead of computing the probability of a\\\\nword given its entire history, we can approximate the history by just the last ...\",\"When we use a bigram model to predict the conditional probability of the next word,\\\\nwe are thus making the following approximation:\\\\nP(wnjw1:n\\\\u00001)\\\\u0019P(wnjwn\\\\u00001) (3.7)\\\\nThe assumption that the probability of a word depends only on the previous word is\\\\ncalled a Markov assumption. Markov models are the class of probabilistic models Markov\\\\nthat assume we can predict the probability of some future unit without looking too\\\\nfar into the past. We can generalize the bigram (which looks one word into the past)\\\\n...\",\"probability of a word given its entire context as follows:\\\\nP(wnjw1:n\\\\u00001)\\\\u0019P(wnjwn\\\\u0000N+1:n\\\\u00001) (3.8)\\\\nGiven the bigram assumption for the probability of an individual word, we can com-\\\\npute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:\\\\nP(w1:n)\\\\u0019nY\\\\nk=1P(wkjwk\\\\u00001) (3.9)3.1 \\\\u2022 N-G RAMS 35\\\\n3.1.2 How to estimate probabilities\\\\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\\\\nestimate probabilities is called maximum likelihood estimation orMLE . We ...\",\"previous word wn\\\\u00001, we\\\\u2019ll compute the count of the bigram C(wn\\\\u00001wn)and normal-\\\\nize by the sum of all the bigrams that share the same \\\\ufb01rst word wn\\\\u00001:\\\\nP(wnjwn\\\\u00001) =C(wn\\\\u00001wn)P\\\\nwC(wn\\\\u00001w)(3.10)\\\\nWe can simplify this equation, since the sum of all bigram counts that start with\\\\na given word wn\\\\u00001must be equal to the unigram count for that word wn\\\\u00001(the reader\\\\nshould take a moment to be convinced of this):\\\\nP(wnjwn\\\\u00001) =C(wn\\\\u00001wn)\\\\nC(wn\\\\u00001)(3.11)\\\\nLet\\\\u2019s work through an example using a mini-corpus of three senten...\",\"3=0:33 P(am|I) =2\\\\n3=0:67\\\\nP(&lt;/s&gt;|Sam ) =1\\\\n2=0:5 P(Sam|am ) =1\\\\n2=0:5 P(do|I) =1\\\\n3=0:33\\\\nFor the general case of MLE n-gram parameter estimation:\\\\nP(wnjwn\\\\u0000N+1:n\\\\u00001) =C(wn\\\\u0000N+1:n\\\\u00001wn)\\\\nC(wn\\\\u0000N+1:n\\\\u00001)(3.12)\\\\nEquation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\\\\nobserved frequency of a particular sequence by the observed frequency of a pre\\\\ufb01x.\\\\nThis ratio is called a relative frequency . We said above that this use of relativerelative\\\\nfrequency\\\\nfrequencies as a way to estimate probabil...\",\"word Chinese ? The MLE of its probability is400\\\\n1000000or 0:0004. Now 0 :0004 is not\\\\nthe best possible estimate of the probability of Chinese occurring in all situations; it\\\\n1We need the end-symbol to make the bigram grammar a true probability distribution. Without an end-\\\\nsymbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities\\\\nfor all sentences of a given length would sum to one. This model would de\\\\ufb01ne an in\\\\ufb01nite set of probability\\\\ndistributions...\",\"Let\\\\u2019s move on to some examples from a real but tiny corpus, drawn from the\\\\nnow-defunct Berkeley Restaurant Project, a dialogue system from the last century\\\\nthat answered questions about a database of restaurants in Berkeley, California (Ju-\\\\nrafsky et al., 1994). Here are some sample user queries (text-normalized, by lower\\\\ncasing and with punctuation striped) (a sample of 9332 sentences is on the website):\\\\ncan you tell me about any good cantonese restaurants close by\\\\ntell me about chez panisse\\\\ni\\\\u2019...\",\"i want to eat chinese food lunch spend\\\\ni 5 827 0 9 0 0 0 2\\\\nwant 2 0 608 1 6 6 5 1\\\\nto 2 0 4 686 2 0 6 211\\\\neat 0 0 2 0 16 2 42 0\\\\nchinese 1 0 0 0 0 82 1 0\\\\nfood 15 0 15 0 1 4 0 0\\\\nlunch 2 0 0 0 0 1 0 0\\\\nspend 1 0 1 0 0 0 0 0\\\\nFigure 3.1 Bigram counts for eight of the words (out of V=1446) in the Berkeley Restau-\\\\nrant Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of\\\\nthe column label word following the row label word. Thus the cell in row iand column want\\\\nmeans that...\",\"Now we can compute the probability of sentences like I want English food or\\\\nI want Chinese food by simply multiplying the appropriate bigram probabilities to-\\\\ngether, as follows:\\\\nP(&lt;s&gt; i want english food &lt;/s&gt; )\\\\n=P(i|&lt;s&gt; )P(want|i )P(english|want )\\\\nP(food|english )P(&lt;/s&gt;|food )\\\\n=0:25\\\\u00020:33\\\\u00020:0011\\\\u00020:5\\\\u00020:68\\\\n=0:0000313.1 \\\\u2022 N-G RAMS 37\\\\ni want to eat chinese food lunch spend\\\\ni 0.002 0.33 0 0.0036 0 0 0 0.00079\\\\nwant 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011\\\\nto 0.00083 0 0.0017 0.28 0.00083 0 0.0...\",\"What kinds of linguistic phenomena are captured in these bigram statistics?\\\\nSome of the bigram probabilities above encode some facts that we think of as strictly\\\\nsyntactic in nature, like the fact that what comes after eatis usually a noun or an\\\\nadjective, or that what comes after tois usually a verb. Others might be a fact about\\\\nthe personal assistant task, like the high probability of sentences beginning with\\\\nthe words I. And some might even be cultural rather than linguistic, like the higher\\\\n...\",\"than or equal to 1, and so the more probabilities we multiply together, the smaller the\\\\nproduct becomes. Multiplying enough n-grams together would result in numerical\\\\nunder\\\\ufb02ow. Adding in log space is equivalent to multiplying in linear space, so we\\\\ncombine log probabilities by adding them. By adding log probabilities instead of\\\\nmultiplying probabilities, we get results that are not as small. We do all computation\\\\nand storage in log space, and just convert back into probabilities if we need to re...\",\"condition on the previous two words, or 4-gram or5-gram models. For these larger 4-gram\\\\n5-gram n-grams, we\\\\u2019ll need to assume extra contexts to the left and right of the sentence end.\\\\nFor example, to compute trigram probabilities at the very beginning of the sentence,\\\\nwe use two pseudo-words for the \\\\ufb01rst trigram (i.e., P(I|&lt;s&gt;&lt;s&gt; ).\\\\nSome large n-gram datasets have been created, like the million most frequent\\\\nn-grams drawn from the Corpus of Contemporary American English (COCA), a\\\\ncurated 1 billio...\",\"(\\\\u00a5-gram) project (Liu et al., 2024) allows n-grams of any length. Their idea is to\\\\navoid the expensive (in space and time) pre-computation of huge n-gram count ta-\\\\nbles. Instead, n-gram probabilities with arbitrary n are computed quickly at inference\\\\ntime by using an ef\\\\ufb01cient representation called suf\\\\ufb01x arrays. This allows computing\\\\nof n-grams of every length for enormous corpora of 5 trillion tokens.\\\\nEf\\\\ufb01ciency considerations are important when building large n-gram language\\\\nmodels. It is standa...\",\"important n-grams (Stolcke, 1998). Ef\\\\ufb01cient language model toolkits like KenLM\\\\n(Hea\\\\ufb01eld 2011, Hea\\\\ufb01eld et al. 2013) use sorted arrays and use merge sorts to ef\\\\ufb01-\\\\nciently build the probability tables in a minimal number of passes through a large\\\\ncorpus.\\\\n3.2 Evaluating Language Models: Training and Test Sets\\\\nThe best way to evaluate the performance of a language model is to embed it in\\\\nan application and measure how much the application improves. Such end-to-end\\\\nevaluation is called extrinsic evalu...\",\"recognizer or machine translator twice, once with each language model, and seeing\\\\nwhich gives the more accurate transcription.\\\\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\\\\nstead, it\\\\u2019s helpful to have a metric that can be used to quickly evaluate potential\\\\nimprovements in a language model. An intrinsic evaluation metric is one that mea-intrinsic\\\\nevaluation\\\\nsures the quality of a model independent of any application. In the next section we\\\\u2019ll\\\\nintroduce perplexity ...\",\"set\\\\ntest setThe training set is the data we use to learn the parameters of our model; for\\\\nsimple n-gram language models it\\\\u2019s the corpus from which we get the counts that\\\\nwe normalize into the probabilities of the n-gram language model.\\\\nThetest set is a different, held-out set of data, not overlapping with the training\\\\nset, that we use to evaluate the model. We need a separate test set to give us an\\\\nunbiased estimate of how well the model we trained can generalize when we apply\\\\nit to some new unk...\",\"How should we choose a training and test set? The test set should re\\\\ufb02ect the\\\\nlanguage we want to use the model for. If we\\\\u2019re going to use our language model3.3 \\\\u2022 E VALUATING LANGUAGE MODELS : PERPLEXITY 39\\\\nfor speech recognition of chemistry lectures, the test set should be text of chemistry\\\\nlectures. If we\\\\u2019re going to use it as part of a system for translating hotel booking re-\\\\nquests from Chinese to English, the test set should be text of hotel booking requests.\\\\nIf we want our language model t...\",\"since that wouldn\\\\u2019t be a good measure of general performance.\\\\nThus if we are given a corpus of text and want to compare the performance of\\\\ntwo different n-gram models, we divide the data into training and test sets, and train\\\\nthe parameters of both models on the training set. We can then compare how well\\\\nthe two trained models \\\\ufb01t the test set.\\\\nBut what does it mean to \\\\u201c\\\\ufb01t the test set\\\\u201d? The standard answer is simple:\\\\nwhichever language model assigns a higher probability to the test set\\\\u2014which\\\\nmea...\",\"probability of a particular \\\\u201ctest\\\\u201d sentence. If our test sentence is part of the training\\\\ncorpus, we will mistakenly assign it an arti\\\\ufb01cially high probability when it occurs\\\\nin the test set. We call this situation training on the test set . Training on the test\\\\nset introduces a bias that makes the probabilities all look too high, and causes huge\\\\ninaccuracies in perplexity , the probability-based metric we introduce below.\\\\nEven if we don\\\\u2019t train on the test set, if we test our language model on t...\",\"test\\\\ntest set or, devset . We do all our testing on this dataset until the very end, and then\\\\nwe test on the test once to see how good our model is.\\\\nHow do we divide our data into training, development, and test sets? We want\\\\nour test set to be as large as possible, since a small test set may be accidentally un-\\\\nrepresentative, but we also want as much training data as possible. At the minimum,\\\\nwe would want to pick the smallest test set that gives us enough statistical power\\\\nto measure a statis...\",\"words, and so it will be less surprised by (i.e., assign a higher probability to) each\\\\nword when it occurs in the test set. Indeed, a perfect language model would correctly\\\\nguess each next word in a corpus, assigning it a probability of 1, and all the other\\\\nwords a probability of zero. So given a test corpus, a better language model will40 CHAPTER 3 \\\\u2022 N- GRAM LANGUAGE MODELS\\\\nassign a higher probability to it than a worse language model.\\\\nBut in fact, we do not use raw probability as our metric fo...\",\"probability called perplexity , is one of the most important metrics in NLP, used for\\\\nevaluating large language models as well as n-gram models.\\\\nTheperplexity (sometimes abbreviated as PP or PPL) of a language model on a perplexity\\\\ntest set is the inverse probability of the test set (one over the probability of the test\\\\nset), normalized by the number of words (or tokens). For this reason it\\\\u2019s sometimes\\\\ncalled the per-word or per-token perplexity. We normalize by the number of words\\\\nNby taking th...\",\"the data, the better the model . Minimizing perplexity is equivalent to maximizing\\\\nthe test set probability according to the language model. Why does perplexity use\\\\nthe inverse probability? It turns out the inverse arises from the original de\\\\ufb01nition\\\\nof perplexity from cross-entropy rate in information theory; for those interested, the\\\\nexplanation is in the advanced Section 3.7. Meanwhile, we just have to remember\\\\nthat perplexity has an inverse relationship with probability.\\\\nThe details of comput...\",\"perplexity (W) =NvuutNY\\\\ni=11\\\\nP(wijwi\\\\u00001)(3.17)\\\\nWhat we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire\\\\nsequence of words in some test set. Since this sequence will cross many sentence\\\\nboundaries, if our vocabulary includes a between-sentence token &lt;EOS&gt; or separate\\\\nbegin- and end-sentence markers &lt;s&gt; and&lt;/s&gt; then we can include them in the3.3 \\\\u2022 E VALUATING LANGUAGE MODELS : PERPLEXITY 41\\\\nprobability computation. If we do, then we also include one token per sentence in\\\\nthe to...\",\"words from the Wall Street Journal newspaper. We then computed the perplexity of\\\\neach of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for\\\\nbigrams, and the corresponding equation for trigrams. The table below shows the\\\\nperplexity of the 1.5 million word test set according to each of the language models.\\\\nUnigram Bigram Trigram\\\\nPerplexity 962 170 109\\\\nAs we see above, the more information the n-gram gives us about the word\\\\nsequence, the higher the probability the n-gram will ...\",\"a lower perplexity tells us that a language model is a better predictor of the test set.\\\\nNote that in computing perplexities, the language model must be constructed\\\\nwithout any knowledge of the test set, or else the perplexity will be arti\\\\ufb01cially low.\\\\nAnd the perplexity of two language models is only comparable if they use identical\\\\nvocabularies.\\\\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-\\\\nprovement in the performance of a language processing task like speech r...\",\"It turns out that perplexity can also be thought of as the weighted average branch-\\\\ning factor of a language. The branching factor of a language is the number of\\\\npossible next words that can follow any word. For example consider a mini arti\\\\ufb01cial\\\\nlanguage that is deterministic (no probabilities), any word can follow any word, and\\\\nwhose vocabulary consists of only three colors:\\\\nL=fred;blue;greeng (3.18)\\\\nThe branching factor of this language is 3.\\\\nNow let\\\\u2019s make a probabilistic version of the same ...\",\"2For example if we use both begin and end tokens, we would include the end-of-sentence marker &lt;/s&gt;\\\\nbut not the beginning-of-sentence marker &lt;s&gt;in our count of N; This is because the end-sentence token is\\\\nfollowed directly by the begin-sentence token with probability almost 1, so we don\\\\u2019t want the probability\\\\nof that fake transition to in\\\\ufb02uence our perplexity.42 CHAPTER 3 \\\\u2022 N- GRAM LANGUAGE MODELS\\\\nperplexity of AonTis:\\\\nperplexityA(T) = PA(red red red red blue )\\\\u00001\\\\n5\\\\n= \\\\u00121\\\\n3\\\\u00135!\\\\u00001\\\\n5\\\\n=\\\\u00121\\\\n3\\\\u0013\\\\u00001\\\\n=3 (3.19...\",\"is very predictable, i.e. has a high probability. So the probability of the test set will\\\\nbe higher, and since perplexity is inversely related to probability, the perplexity will\\\\nbe lower. Thus, although the branching factor is still 3, the perplexity or weighted\\\\nbranching factor is smaller:\\\\nperplexityB(T) = PB(red red red red blue )\\\\u00001=5\\\\n=0:04096\\\\u00001\\\\n5\\\\n=0:527\\\\u00001=1:89 (3.21)\\\\n3.4 Sampling sentences from a language model\\\\n010.06the.060.03of0.02a0.02toin.09.11.13.15\\\\u2026however(p=0.0003)polyphonicp=0.000001...\",\"and 1, it will fall in an interval corresponding to some word. The expectation for the random\\\\nnumber to fall in the larger intervals of one of the frequent words ( the,of,a) is much higher\\\\nthan in the smaller interval of one of the rare words ( polyphonic ).\\\\nOne important way to visualize what kind of knowledge a language model em-\\\\nbodies is to sample from it. Sampling from a distribution means to choose random sampling\\\\npoints according to their likelihood. Thus sampling from a language model\\\\u2014wh...\",\"This technique of visualizing a language model by sampling was \\\\ufb01rst suggested\\\\nvery early on by Shannon (1948) and Miller and Selfridge (1950). It\\\\u2019s simplest to\\\\nvisualize how this works for the unigram case. Imagine all the words of the English\\\\nlanguage covering the number line between 0 and 1, each word covering an interval\\\\nproportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM\\\\ncomputed from the text of this book. We choose a random value between 0 and 1,\\\\n\\\\ufb01nd that poin...\",\"second word of that bigram is w. We next choose a random bigram starting with w\\\\n(again, drawn according to its bigram probability), and so on.\\\\n3.5 Generalizing vs. over\\\\ufb01tting the training set\\\\nThe n-gram model, like many statistical models, is dependent on the training corpus.\\\\nOne implication of this is that the probabilities often encode speci\\\\ufb01c facts about a\\\\ngiven training corpus. Another implication is that n-grams do a better and better job\\\\nof modeling the training corpus as we increase the v...\",\"rote life have\\\\ngram \\\\u2013Hill he late speaks; or! a more to leg less \\\\ufb01rst you enter\\\\n2\\\\u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\\\\nking. Follow.\\\\ngram \\\\u2013What means, sir. I confess she? then all sorts, he is trim, captain.\\\\n3\\\\u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\\\\n\\\\u2019tis done.\\\\ngram \\\\u2013This shall forbid it should be branded, if renown made it empty.\\\\n4\\\\u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt so...\",\"tences show no coherent relation between words nor any sentence-\\\\ufb01nal punctua-\\\\ntion. The bigram sentences have some local word-to-word coherence (especially\\\\nconsidering punctuation as words). The trigram sentences are beginning to look a\\\\nlot like Shakespeare. Indeed, the 4-gram sentences look a little too much like Shake-\\\\nspeare. The words It cannot be but so are directly from King John . This is because,\\\\nnot to put the knock on Shakespeare, his oeuvre is not very large as corpora go44 CHAPTER 3 ...\",\"To get an idea of the dependence on the training set, let\\\\u2019s look at LMs trained on a\\\\ncompletely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare\\\\nand the WSJ are both English, so we might have expected some overlap between our\\\\nn-grams for the two genres. Fig. 3.5 shows sentences generated by unigram, bigram,\\\\nand trigram grammars trained on 40 million words from WSJ.\\\\n1Months the my and issue of year foreign new exchange\\\\u2019s september\\\\nwere recession exchange new endorsed a acqui...\",\"gram Brazil on market conditions\\\\nFigure 3.5 Three sentences randomly generated from three n-gram models computed from\\\\n40 million words of the Wall Street Journal , lower-casing all characters and treating punctua-\\\\ntion as words. Output was then hand-corrected for capitalization to improve readability.\\\\nCompare these examples to the pseudo-Shakespeare in Fig. 3.4. While they both\\\\nmodel \\\\u201cEnglish-like sentences\\\\u201d, there is no overlap in the generated sentences, and\\\\nlittle overlap even in small phrase...\",\"we need a training corpus of legal documents. To build a language model for a\\\\nquestion-answering system, we need a training corpus of questions.\\\\nIt is equally important to get training data in the appropriate dialect orvariety ,\\\\nespecially when processing social media posts or spoken transcripts. For exam-\\\\nple some tweets will use features of African American English (AAE)\\\\u2014 the name\\\\nfor the many variations of language used in African American communities (King,\\\\n2020). Such features can include w...\",\"(3.23) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\\\\ntweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\\\\nIs it possible for the testset nonetheless to have a word we have never seen be-\\\\nfore? What happens if the word Jurafsky never occurs in our training set, but pops\\\\nup in the test set? The answer is that although words might be unseen, we actu-\\\\nally run our NLP algorithms not on words but on subword tokens . With subword3.6 \\\\u2022 S MOOTHING , INTERPOLATIO...\",\"test set can never contain unseen tokens.\\\\n3.6 Smoothing, Interpolation, and Backoff\\\\nThere is a problem with using maximum likelihood estimates for probabilities: any\\\\n\\\\ufb01nite training corpus will be missing some perfectly acceptable English word se-\\\\nquences. That is, cases where a particular n-gram never occurs in the training data\\\\nbut appears in the test set. Perhaps our training corpus has the words ruby and\\\\nslippers in it but just happens not to have the phrase ruby slippers .\\\\nThese unseen seque...\",\"set is 0. Perplexity is de\\\\ufb01ned based on the inverse probability of the test set. Thus\\\\nif some words in context have zero probability, we can\\\\u2019t compute perplexity at all,\\\\nsince we can\\\\u2019t divide by 0!\\\\nThe standard way to deal with putative \\\\u201czero probability n-grams\\\\u201d that should re-\\\\nally have some non-zero probability is called smoothing ordiscounting . Smoothing smoothing\\\\ndiscounting algorithms shave off a bit of probability mass from some more frequent events and\\\\ngive it to unseen events. Here we\\\\u2019...\",\"Laplace smoothing . Laplace smoothing does not perform well enough to be usedLaplace\\\\nsmoothing\\\\nin modern n-gram models, but it usefully introduces many of the concepts that we\\\\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\\\\nsmoothing algorithm for other tasks like text classi\\\\ufb01cation (Chapter 4).\\\\nLet\\\\u2019s start with the application of Laplace smoothing to unigram probabilities.\\\\nRecall that the unsmoothed maximum likelihood estimate of the unigram probability\\\\nof th...\",\"PLaplace (wi) =ci+1\\\\nN+V(3.24)46 CHAPTER 3 \\\\u2022 N- GRAM LANGUAGE MODELS\\\\nInstead of changing both the numerator and denominator, it is convenient to describe\\\\nhow a smoothing algorithm affects the numerator, by de\\\\ufb01ning an adjusted count c\\\\u0003.\\\\nThis adjusted count is easier to compare directly with the MLE counts and can be\\\\nturned into a probability like an MLE count by normalizing by N. To de\\\\ufb01ne this\\\\ncount, since we are only changing the numerator in addition to adding 1 we\\\\u2019ll also\\\\nneed to multiply by a ...\",\"the original counts:\\\\ndi=c\\\\u0003\\\\ni\\\\nci\\\\nNow that we have the intuition for the unigram case, let\\\\u2019s smooth our Berkeley\\\\nRestaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\\\\nbigrams in Fig. 3.1.\\\\ni want to eat chinese food lunch spend\\\\ni 6 828 1 10 1 1 1 3\\\\nwant 3 1 609 2 7 7 6 2\\\\nto 3 1 5 687 3 1 7 212\\\\neat 1 1 3 1 17 3 43 1\\\\nchinese 2 1 1 1 1 83 2 1\\\\nfood 16 1 16 1 2 5 1 1\\\\nlunch 3 1 1 1 1 2 1 1\\\\nspend 2 1 2 1 1 1 1 1\\\\nFigure 3.6 Add-one smoothed bigram counts for eight of the words (o...\",\"number of total word types in the vocabulary V:\\\\nPLaplace (wnjwn\\\\u00001) =C(wn\\\\u00001wn)+1P\\\\nw(C(wn\\\\u00001w)+1)=C(wn\\\\u00001wn)+1\\\\nC(wn\\\\u00001)+V(3.27)\\\\nThus, each of the unigram counts given in the previous section will need to be aug-\\\\nmented by V=1446. The result is the smoothed bigram probabilities in Fig. 3.7.\\\\nIt is often convenient to reconstruct the count matrix so we can see how much a\\\\nsmoothing algorithm has changed the original counts. These adjusted counts can be3.6 \\\\u2022 S MOOTHING , INTERPOLATION ,AND BACKOFF 47\\\\ni wa...\",\"lunch 0.0017 0.00056 0.00056 0.00056 0.00056 0.0011 0.00056 0.00056\\\\nspend 0.0012 0.00058 0.0012 0.00058 0.00058 0.00058 0.00058 0.00058\\\\nFigure 3.7 Add-one smoothed bigram probabilities for eight of the words (out of V=1446) in the BeRP\\\\ncorpus of 9332 sentences. Previously-zero probabilities are in gray.\\\\ni want to eat chinese food lunch spend\\\\ni 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9\\\\nwant 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78\\\\nto 1.9 0.63 3.1 430 1.9 0.63 4.4 133\\\\neat 0.34 0.34 1 0.34 5.8 1 15 0.34\\\\nchines...\",\"c\\\\u0003(wn\\\\u00001wn) =[C(wn\\\\u00001wn)+1]\\\\u0002C(wn\\\\u00001)\\\\nC(wn\\\\u00001)+V(3.28)\\\\nNote that add-one smoothing has made a very big change to the counts. Com-\\\\nparing Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to )changed\\\\nfrom 608 to 238! We can see this in probability space as well: P(tojwant)decreases\\\\nfrom 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the dis-\\\\ncount d(the ratio between new and old counts) shows us how strikingly the counts\\\\nfor each pre\\\\ufb01x word have been reduced; th...\",\"P\\\\u0003\\\\nAdd-k(wnjwn\\\\u00001) =C(wn\\\\u00001wn)+k\\\\nC(wn\\\\u00001)+kV(3.29)\\\\nAdd-k smoothing requires that we have a method for choosing k; this can be\\\\ndone, for example, by optimizing on a devset . Although add-k is useful for some\\\\ntasks (including text classi\\\\ufb01cation), it turns out that it still doesn\\\\u2019t work well for\\\\nlanguage modeling, generating counts with poor variances and often inappropriate\\\\ndiscounts (Gale and Church, 1994).48 CHAPTER 3 \\\\u2022 N- GRAM LANGUAGE MODELS\\\\n3.6.3 Language Model Interpolation\\\\nThere is an alternat...\",\"hasn\\\\u2019t learned much about.\\\\nThe most common way to use this n-gram hierarchy is called interpolation : interpolation\\\\ncomputing a new probability by interpolating (weighting and combining) the tri-\\\\ngram, bigram, and unigram probabilities.3In simple linear interpolation, we com-\\\\nbine different order n-grams by linearly interpolating them. Thus, we estimate the\\\\ntrigram probability P(wnjwn\\\\u00002wn\\\\u00001)by mixing together the unigram, bigram, and\\\\ntrigram probabilities, each weighted by a l:\\\\n\\\\u02c6P(wnjwn\\\\u00002wn\\\\u00001) =...\",\"this bigram will be more trustworthy, so we can make the ls for those trigrams\\\\nhigher and thus give that trigram more weight in the interpolation. Equation 3.31\\\\nshows the equation for interpolation with context-conditioned weights, where each\\\\nlambda takes an argument that is the two prior word context:\\\\n\\\\u02c6P(wnjwn\\\\u00002wn\\\\u00001) = l1(wn\\\\u00002:n\\\\u00001)P(wn)\\\\n+l2(wn\\\\u00002:n\\\\u00001)P(wnjwn\\\\u00001)\\\\n+l3(wn\\\\u00002:n\\\\u00001)P(wnjwn\\\\u00002wn\\\\u00001) (3.31)\\\\nHow are these lvalues set? Both the simple interpolation and conditional interpo-\\\\nlation ls are learn...\",\"of the held-out set. There are various ways to \\\\ufb01nd this optimal set of ls. One way\\\\nis to use the EMalgorithm, an iterative learning algorithm that converges on locally\\\\noptimal ls (Jelinek and Mercer, 1980).\\\\n3.6.4 Stupid Backoff\\\\nAn alternative to interpolation is backoff . In a backoff model, if the n-gram we need backoff\\\\n3We won\\\\u2019t discuss the less-common alternative, called backoff , in which we use the trigram if the\\\\nevidence is suf\\\\ufb01cient for it, but if not we instead just use the bigram, other...\",\"has zero counts, we approximate it by backing off to the (n-1)-gram. We continue\\\\nbacking off until we reach a history that has some counts. For a backoff model to\\\\ngive a correct probability distribution, we have to discount the higher-order n-grams discount\\\\nto save some probability mass for the lower order n-grams. In practice, instead of\\\\ndiscounting, it\\\\u2019s common to use a much simpler non-discounted backoff algorithm\\\\ncalled stupid backoff (Brants et al., 2007). stupid backoff\\\\nStupid backoff give...\",\"S(wijwi\\\\u0000N+1:i\\\\u00001) =8\\\\n&lt;\\\\n:count (wi\\\\u0000N+1:i)\\\\ncount (wi\\\\u0000N+1:i\\\\u00001)if count (wi\\\\u0000N+1:i)&gt;0\\\\nlS(wijwi\\\\u0000N+2:i\\\\u00001)otherwise(3.32)\\\\nThe backoff terminates in the unigram, which has score S(w) =count (w)\\\\nN. Brants et al.\\\\n(2007) \\\\ufb01nd that a value of 0.4 worked well for l.\\\\n3.7 Advanced: Perplexity\\\\u2019s Relation to Entropy\\\\nWe introduced perplexity in Section 3.3 as a way to evaluate n-gram models on\\\\na test set. A better n-gram model is one that assigns a higher probability to the\\\\ntest data, and perplexity is a normalized ...\",\"predicting (words, letters, parts of speech), the set of which we\\\\u2019ll call c, and with a\\\\nparticular probability function, call it p(x), the entropy of the random variable Xis:\\\\nH(X) =\\\\u0000X\\\\nx2cp(x)log2p(x) (3.33)\\\\nThe log can, in principle, be computed in any base. If we use log base 2, the\\\\nresulting value of entropy will be measured in bits.\\\\nOne intuitive way to think about entropy is as a lower bound on the number of\\\\nbits it would take to encode a certain decision or piece of information in the optim...\",\"as the code; thus, horse 1 would be 001, horse 2010, horse 3011, and so on, with\\\\nhorse 8 coded as 000. If we spend the whole day betting and each horse is coded\\\\nwith 3 bits, on average we would be sending 3 bits per race.\\\\nCan we do better? Suppose that the spread is the actual distribution of the bets\\\\nplaced and that we represent it as the prior probability of each horse as follows:50 CHAPTER 3 \\\\u2022 N- GRAM LANGUAGE MODELS\\\\nHorse 11\\\\n2Horse 51\\\\n64\\\\nHorse 21\\\\n4Horse 61\\\\n64\\\\nHorse 31\\\\n8Horse 71\\\\n64\\\\nHorse 41\\\\n1...\",\"could encode the most likely horse with the code 0, and the remaining horses as 10,\\\\nthen110,1110 ,111100 ,111101 ,111110 , and111111 .\\\\nWhat if the horses are equally likely? We saw above that if we used an equal-\\\\nlength binary code for the horse numbers, each horse took 3 bits to code, so the\\\\naverage was 3. Is the entropy the same? In this case each horse would have a\\\\nprobability of1\\\\n8. The entropy of the choice of horses is then\\\\nH(X) =\\\\u0000i=8X\\\\ni=11\\\\n8log21\\\\n8=\\\\u0000log21\\\\n8=3 bits (3.35)\\\\nUntil now we have...\",\"H(w1;w2;:::; wn) =\\\\u0000X\\\\nw1:n2Lp(w1:n)logp(w1:n) (3.36)\\\\nWe could de\\\\ufb01ne the entropy rate (we could also think of this as the per-word entropy rate\\\\nentropy ) as the entropy of this sequence divided by the number of words:\\\\n1\\\\nnH(w1:n) =\\\\u00001\\\\nnX\\\\nw1:n2Lp(w1:n)logp(w1:n) (3.37)\\\\nBut to measure the true entropy of a language, we need to consider sequences of\\\\nin\\\\ufb01nite length. If we think of a language as a stochastic process Lthat produces a\\\\nsequence of words, and allow Wto represent the sequence of words w1;:::;...\",\"That is, we can take a single sequence that is long enough instead of summing over\\\\nall possible sequences. The intuition of the Shannon-McMillan-Breiman theorem\\\\nis that a long-enough sequence of words will contain in it many other shorter se-\\\\nquences and that each of these shorter sequences will reoccur in the longer sequence\\\\naccording to their probabilities.\\\\nA stochastic process is said to be stationary if the probabilities it assigns to a Stationary\\\\nsequence are invariant with respect to shift...\",\"in Appendix D, the probability of upcoming words can be dependent on events that\\\\nwere arbitrarily distant and time dependent. Thus, our statistical models only give\\\\nan approximation to the correct distributions and entropies of natural language.\\\\nTo summarize, by making some incorrect but convenient simplifying assump-\\\\ntions, we can compute the entropy of some stochastic process by taking a very long\\\\nsample of the output and computing its average log probability.\\\\nNow we are ready to introduce cro...\",\"log of their probabilities according to m.\\\\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary er-\\\\ngodic process:\\\\nH(p;m) =lim\\\\nn!\\\\u00a5\\\\u00001\\\\nnlogm(w1w2:::wn) (3.41)\\\\nThis means that, as for entropy, we can estimate the cross-entropy of a model m\\\\non some distribution pby taking a single sequence that is long enough instead of\\\\nsumming over all possible sequences.\\\\nWhat makes the cross-entropy useful is that the cross-entropy H(p;m)is an up-\\\\nper bound on the entropy H(p). For any model m:\\\\n...\",\"lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\\\\na model cannot err by underestimating the true entropy.)\\\\nWe are \\\\ufb01nally ready to see the relation between perplexity and cross-entropy\\\\nas we saw it in Eq. 3.41. Cross-entropy is de\\\\ufb01ned in the limit as the length of the\\\\nobserved word sequence goes to in\\\\ufb01nity. We approximate this cross-entropy by\\\\nrelying on a (suf\\\\ufb01ciently long) sequence of \\\\ufb01xed length. This approximation to the\\\\ncross-entropy of a model M=P(wijwi\\\\u0000N+...\",\"that allows us to introduce many of the basic concepts in language modeling.\\\\n\\\\u2022 Language models offer a way to assign a probability to a sentence or other\\\\nsequence of words or tokens, and to predict a word or token from preceding\\\\nwords or tokens.\\\\n\\\\u2022N-grams are perhaps the simplest kind of language model. They are Markov\\\\nmodels that estimate words from a \\\\ufb01xed window of previous words. N-gram\\\\nmodels can be trained by counting in a training corpus and normalizing the\\\\ncounts (the maximum likelihood es...\",\"\\\\u2022Smoothing algorithms provide a way to estimate probabilities for events that\\\\nwere unseen in training. Commonly used smoothing algorithms for n-grams\\\\ninclude add-1 smoothing, or rely on lower-order n-gram counts through inter-\\\\npolation .\\\\nBibliographical and Historical Notes\\\\nThe underlying mathematics of the n-gram was \\\\ufb01rst proposed by Markov (1913),\\\\nwho used what are now called Markov chains (bigrams and trigrams) to predict\\\\nwhether an upcoming letter in Pushkin\\\\u2019s Eugene Onegin would be a vowel ...\",\"engineering, linguistic, and psychological work on modeling word sequences by the\\\\n1950s. In a series of extremely in\\\\ufb02uential papers starting with Chomsky (1956) and\\\\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\\\\nthat \\\\u201c\\\\ufb01nite-state Markov processes\\\\u201d, while a possibly useful engineering heuristic,\\\\nwere incapable of being a complete cognitive model of human grammatical knowl-\\\\nedge. These arguments led many linguists and computational linguists to ignore\\\\nwork in statistic...\",\"labs successfully used n-grams in their speech recognition systems at the same time\\\\n(Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The\\\\nterms \\\\u201clanguage model\\\\u201d and \\\\u201cperplexity\\\\u201d were \\\\ufb01rst used for this technology by the\\\\nIBM group. Jelinek and his colleagues used the term language model in a pretty\\\\nmodern way, to mean the entire set of linguistic in\\\\ufb02uences on word sequence prob-\\\\nabilities, including grammar, semantics, discourse, and even speaker characteristics,\\\\nra...\",\"A wide variety of different language modeling and smoothing techniques were\\\\nproposed in the 80s and 90s, including Good-Turing discounting\\\\u2014\\\\ufb01rst applied to the\\\\nn-gram smoothing at IBM by Katz (N \\\\u00b4adas 1984, Church and Gale 1991)\\\\u2014 Witten-\\\\nBell discounting (Witten and Bell, 1991), and varieties of class-based n-gram mod-class-based\\\\nn-gram\\\\nels that used information about word classes. Starting in the late 1990s, Chen and\\\\nGoodman performed a number of carefully controlled experiments comparing dif-\\\\nf...\",\"gram language models.\\\\nLarge language models are based on neural networks rather than n-grams, en-\\\\nabling them to solve the two major problems with n-grams: (1) the number of param-\\\\neters increases exponentially as the n-gram order increases, and (2) n-grams have no\\\\nway to generalize from training examples to test set examples unless they use iden-\\\\ntical words. Neural language models instead project words into a continuous space\\\\nin which words with similar contexts have similar representations. W...\",\"Now write out all the non-zero trigram probabilities for the I am Sam corpus\\\\non page 35.\\\\n3.2 Calculate the probability of the sentence i want chinese food . Give two\\\\nprobabilities, one using Fig. 3.2 and the \\\\u2018useful probabilities\\\\u2019 just below it on\\\\npage 37, and another using the add-1 smoothed table in Fig. 3.7. Assume the\\\\nadditional add-1 smoothed probabilities P(i|&lt;s&gt; ) =0:19 and P(&lt;/s&gt;|food ) =\\\\n0:40.\\\\n3.3 Which of the two probabilities you computed in the previous exercise is higher,\\\\nunsmoothed...\",\"grammar on the following training corpus without using the end-symbol &lt;/s&gt; :\\\\n&lt;s&gt; a b\\\\n&lt;s&gt; b b\\\\n&lt;s&gt; b a\\\\n&lt;s&gt; a a\\\\nDemonstrate that your bigram model does not assign a single probability dis-\\\\ntribution across all sentence lengths by showing that the sum of the probability\\\\nof the four possible 2 word sentences over the alphabet fa,bgis 1.0, and the\\\\nsum of the probability of all possible 3 word sentences over the alphabet fa,bg\\\\nis also 1.0.\\\\n3.6 Suppose we train a trigram language model with add-one smoo...\",\"&lt;s&gt; I am Sam &lt;/s&gt;\\\\n&lt;s&gt; Sam I am &lt;/s&gt;\\\\n&lt;s&gt; I am Sam &lt;/s&gt;\\\\n&lt;s&gt; I do not like green eggs and Sam &lt;/s&gt;\\\\nIf we use linear interpolation smoothing between a maximum-likelihood bi-\\\\ngram model and a maximum-likelihood unigram model with l1=1\\\\n2andl2=\\\\n1\\\\n2, what is P(Samjam)? Include &lt;s&gt; and&lt;/s&gt; in your counts just like any\\\\nother token.\\\\n3.8 Write a program to compute unsmoothed unigrams and bigrams.EXERCISES 55\\\\n3.9 Run your n-gram program on two different small corpora of your choice (you\\\\nmight use email text ...\",\"each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0\\\\n0 0. What is the unigram perplexity?56 CHAPTER 4 \\\\u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\\\\nCHAPTER\\\\n4Naive Bayes, Text Classi\\\\ufb01ca-\\\\ntion, and Sentiment\\\\nClassi\\\\ufb01cation lies at the heart of both human and machine intelligence. Deciding\\\\nwhat letter, word, or image has been presented to our senses, recognizing faces\\\\nor voices, sorting mail, assigning grades to homeworks; these are all examples of\\\\nassigning a categ...\",\"tremble as if they were mad, (j) innumerable ones, (k) those drawn with\\\\na very \\\\ufb01ne camel\\\\u2019s hair brush, (l) others, (m) those that have just broken\\\\na \\\\ufb02ower vase, (n) those that resemble \\\\ufb02ies from a distance.\\\\nMany language processing tasks involve classi\\\\ufb01cation, although luckily our classes\\\\nare much easier to de\\\\ufb01ne than those of Borges. In this chapter we introduce the naive\\\\nBayes algorithm and apply it to text categorization , the task of assigning a label ortext\\\\ncategorization\\\\ncategory to an ent...\",\"sentiment toward a candidate or political action. Extracting consumer or public sen-\\\\ntiment is thus relevant for \\\\ufb01elds from marketing to politics.\\\\nThe simplest version of sentiment analysis is a binary classi\\\\ufb01cation task, and\\\\nthe words of the review provide excellent cues. Consider, for example, the follow-\\\\ning phrases extracted from positive and negative reviews of movies and restaurants.\\\\nWords like great ,richly ,awesome , and pathetic , and awful andridiculously are very\\\\ninformative cues:\\\\n+.....\",\"si\\\\ufb01cation task of assigning an email to one of the two classes spam ornot-spam .\\\\nMany lexical and other features can be used to perform this classi\\\\ufb01cation. For ex-\\\\nample you might quite reasonably be suspicious of an email containing phrases like\\\\n\\\\u201conline pharmaceutical\\\\u201d or \\\\u201cWITHOUT ANY COST\\\\u201d or \\\\u201cDear Winner\\\\u201d.\\\\nAnother thing we might want to know about a text is the language it\\\\u2019s written\\\\nin. Texts on social media, for example, can be in any number of languages and\\\\nwe\\\\u2019ll need to apply different pro...\",\"Finally, one of the oldest tasks in text classi\\\\ufb01cation is assigning a library sub-\\\\nject category or topic label to a text. Deciding whether a research paper concerns\\\\nepidemiology or instead, perhaps, embryology, is an important component of infor-\\\\nmation retrieval. Various sets of subject categories exist, such as the MeSH (Medical\\\\nSubject Headings) thesaurus. In fact, as we will see, subject category classi\\\\ufb01cation\\\\nis the task for which the naive Bayes algorithm was invented in 1961 Maron (1961)...\",\"context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17)\\\\nclassi\\\\ufb01es each occurrence of a word in a sentence as, e.g., a noun or a verb.\\\\nThe goal of classi\\\\ufb01cation is to take a single observation, extract some useful\\\\nfeatures, and thereby classify the observation into one of a set of discrete classes.\\\\nOne method for classifying text is to use rules handwritten by humans. Handwrit-\\\\nten rule-based classi\\\\ufb01ers can be components of state-of-the-art systems in language\\\\nprocess...\",\"learninglearning, we have a data set of input observations, each associated with some correct\\\\noutput (a \\\\u2018supervision signal\\\\u2019). The goal of the algorithm is to learn how to map\\\\nfrom a new observation to a correct output.\\\\nFormally, the task of supervised classi\\\\ufb01cation is to take an input xand a \\\\ufb01xed\\\\nset of output classes Y=fy1;y2;:::;yMgand return a predicted class y2Y. For\\\\ntext classi\\\\ufb01cation, we\\\\u2019ll sometimes talk about c(for \\\\u201cclass\\\\u201d) instead of yas our\\\\noutput variable, and d(for \\\\u201cdocument\\\\u201d) inste...\",\"us the probability of the observation being in the class. This full distribution over\\\\nthe classes can be useful information for downstream decisions; avoiding making\\\\ndiscrete decisions early on can be useful when combining systems.\\\\nMany kinds of machine learning algorithms are used to build classi\\\\ufb01ers. This\\\\nchapter introduces naive Bayes; the following one introduces logistic regression.\\\\nThese exemplify two ways of doing classi\\\\ufb01cation. Generative classi\\\\ufb01ers like naive\\\\nBayes build a model of how ...\",\"generative classi\\\\ufb01ers still have a role.\\\\n4.1 Naive Bayes Classi\\\\ufb01ers\\\\nIn this section we introduce the multinomial naive Bayes classi\\\\ufb01er , so called be-naive Bayes\\\\nclassi\\\\ufb01er\\\\ncause it is a Bayesian classi\\\\ufb01er that makes a simplifying (naive) assumption about58 CHAPTER 4 \\\\u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\\\\nhow the features interact.\\\\nThe intuition of the classi\\\\ufb01er is shown in Fig. 4.1. We represent a text document\\\\nas if it were a bag of words , that is, an unordered set of words with t...\",\"ititititititIIII\\\\nIloverecommendmoviethethethetheto\\\\ntotoand\\\\nandandseenseenyetwouldwithwhowhimsical\\\\nwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It&#x27;s sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone....\",\"words is ignored (the bag-of-words assumption) and we make use of the frequency of each word.\\\\nNaive Bayes is a probabilistic classi\\\\ufb01er, meaning that for a document d, out of\\\\nall classes c2Cthe classi\\\\ufb01er returns the class \\\\u02c6 cwhich has the maximum posterior\\\\nprobability given the document. In Eq. 4.1 we use the hat notation \\\\u02c6to mean \\\\u201cour \\\\u02c6\\\\nestimate of the correct class\\\\u201d, and we use argmax to mean an operation that selects argmax\\\\nthe argument (in this case the class c) that maximizes a function (in ...\",\"Eq. 4.2; it gives us a way to break down any conditional probability P(xjy)into\\\\nthree other probabilities:\\\\nP(xjy) =P(yjx)P(x)\\\\nP(y)(4.2)4.1 \\\\u2022 N AIVE BAYES CLASSIFIERS 59\\\\nWe can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:\\\\n\\\\u02c6c=argmax\\\\nc2CP(cjd) =argmax\\\\nc2CP(djc)P(c)\\\\nP(d)(4.3)\\\\nWe can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This\\\\nis possible because we will be computingP(djc)P(c)\\\\nP(d)for each possible class. But P(d)\\\\ndoesn\\\\u2019t change for each class; we are always asking ab...\",\"sampled from P(c), and then the words are generated by sampling from P(djc). (In\\\\nfact we could imagine generating arti\\\\ufb01cial documents, or at least their word counts,\\\\nby following this process). We\\\\u2019ll say more about this intuition of generative models\\\\nin Chapter 5.\\\\nTo return to classi\\\\ufb01cation: we compute the most probable class \\\\u02c6 cgiven some\\\\ndocument dby choosing the class which has the highest product of two probabilities:\\\\ntheprior probability of the class P(c)and the likelihood of the document P...\",\"features (for example, every possible set of words and positions) would require huge\\\\nnumbers of parameters and impossibly large training sets. Naive Bayes classi\\\\ufb01ers\\\\ntherefore make two simplifying assumptions.\\\\nThe \\\\ufb01rst is the bag-of-words assumption discussed intuitively above: we assume\\\\nposition doesn\\\\u2019t matter, and that the word \\\\u201clove\\\\u201d has the same effect on classi\\\\ufb01cation\\\\nwhether it occurs as the 1st, 20th, or last word in the document. Thus we assume\\\\nthat the features f1;f2;:::;fnonly encode w...\",\"The \\\\ufb01nal equation for the class chosen by a naive Bayes classi\\\\ufb01er is thus:\\\\ncNB=argmax\\\\nc2CP(c)Y\\\\nf2FP(fjc) (4.8)60 CHAPTER 4 \\\\u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\\\\nTo apply the naive Bayes classi\\\\ufb01er to text, we will use each word in the documents\\\\nas a feature, as suggested above, and we consider each of the words in the document\\\\nby walking an index through every word position in the document:\\\\npositions all word positions in test document\\\\ncNB=argmax\\\\nc2CP(c)Y\\\\ni2positionsP(wijc) (4.9)\\\\nNa...\",\"to make a classi\\\\ufb01cation decision \\\\u2014like naive Bayes and also logistic regression\\\\u2014\\\\nare called linear classi\\\\ufb01ers .linear\\\\nclassi\\\\ufb01ers\\\\n4.2 Training the Naive Bayes Classi\\\\ufb01er\\\\nHow can we learn the probabilities P(c)andP(fijc)? Let\\\\u2019s \\\\ufb01rst consider the maxi-\\\\nmum likelihood estimate. We\\\\u2019ll simply use the frequencies in the data. For the class\\\\nprior P(c)we ask what percentage of the documents in our training set are in each\\\\nclass c. Let Ncbe the number of documents in our training data with class cand\\\\nNdocb...\",\"Then we use the frequency of wiin this concatenated document to give a maximum\\\\nlikelihood estimate of the probability:\\\\n\\\\u02c6P(wijc) =count (wi;c)P\\\\nw2Vcount (w;c)(4.12)\\\\nHere the vocabulary V consists of the union of all the word types in all classes, not\\\\njust the words in one class c.\\\\nThere is a problem, however, with maximum likelihood training. Imagine we\\\\nare trying to estimate the likelihood of the word \\\\u201cfantastic\\\\u201d given class positive , but\\\\nsuppose there are no training documents that both contai...\",\"But since naive Bayes naively multiplies all the feature likelihoods together, zero\\\\nprobabilities in the likelihood term for any class will cause the probability of the\\\\nclass to be zero, no matter the other evidence!\\\\nThe simplest solution is the add-one (Laplace) smoothing introduced in Chap-\\\\nter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing\\\\nalgorithms in language modeling, it is commonly used in naive Bayes text catego-\\\\nrization:\\\\n\\\\u02c6P(wijc) =count (wi;c)+1P\\\\nw2V(co...\",\"ulary at all because they did not occur in any training document in any class? The\\\\nsolution for such unknown words is to ignore them\\\\u2014remove them from the test unknown word\\\\ndocument and not include any probability for them at all.\\\\nFinally, some systems choose to completely ignore another class of words: stop\\\\nwords , very frequent words like theanda. This can be done by sorting the vocabu- stop words\\\\nlary by frequency in the training set, and de\\\\ufb01ning the top 10\\\\u2013100 vocabulary entries\\\\nas stop words...\",\"list.\\\\nFig. 4.2 shows the \\\\ufb01nal algorithm.\\\\n4.3 Worked example\\\\nLet\\\\u2019s walk through an example of training and testing naive Bayes with add-one\\\\nsmoothing. We\\\\u2019ll use a sentiment analysis domain with the two classes positive\\\\n(+) and negative (-), and take the following miniature training and test documents\\\\nsimpli\\\\ufb01ed from actual movie reviews.\\\\nCat Documents\\\\nTraining - just plain boring\\\\n- entirely predictable and lacks energy\\\\n- no surprises and very few laughs\\\\n+ very powerful\\\\n+ the most fun \\\\ufb01lm of the su...\",\"function TRAIN NAIVE BAYES (D, C) returns V;logP(c), log P(wjc)\\\\nfor each class c2C # Calculate P(c)terms\\\\nNdoc= number of documents in D\\\\nNc= number of documents from D in class c\\\\nlogprior [c] logNc\\\\nNdoc\\\\nV vocabulary of D\\\\nbigdoc [c] append (d)ford2Dwith class c\\\\nfor each word win V # Calculate P(wjc)terms\\\\ncount(w,c) # of occurrences of winbigdoc [c]\\\\nloglikelihood [w,c] logcount (w;c) + 1P\\\\nw0in V(count (w0;c) + 1)\\\\nreturn logprior ,loglikelihood ,V\\\\nfunction TESTNAIVE BAYES (testdoc ,logprior ,loglike...\",\"of the words in the training set is left as an exercise for the reader):\\\\nP(\\\\u201cpredictable\\\\u201dj\\\\u0000) =1+1\\\\n14+20P(\\\\u201cpredictable\\\\u201dj+) =0+1\\\\n9+20\\\\nP(\\\\u201cno\\\\u201dj\\\\u0000) =1+1\\\\n14+20P(\\\\u201cno\\\\u201dj+) =0+1\\\\n9+20\\\\nP(\\\\u201cfun\\\\u201dj\\\\u0000) =0+1\\\\n14+20P(\\\\u201cfun\\\\u201dj+) =1+1\\\\n9+20\\\\nFor the test sentence S = \\\\u201cpredictable with no fun\\\\u201d, after removing the word \\\\u2018with\\\\u2019,\\\\nthe chosen class, via Eq. 4.9, is therefore computed as follows:\\\\nP(\\\\u0000)P(Sj\\\\u0000) =3\\\\n5\\\\u00022\\\\u00022\\\\u00021\\\\n343=6:1\\\\u000210\\\\u00005\\\\nP(+)P(Sj+) =2\\\\n5\\\\u00021\\\\u00021\\\\u00022\\\\n293=3:2\\\\u000210\\\\u00005\\\\nThe model thus predicts the class negative for the test sentence.\\\\n4...\",\"improves performance to clip the word counts in each document at 1 (see the end\\\\nof the chapter for pointers to these results). This variant is called binary multino-\\\\nmial naive Bayes orbinary naive Bayes . The variant uses the same algorithm asbinary naive\\\\nBayes\\\\nin Fig. 4.2 except that for each document we remove all duplicate words before con-\\\\ncatenating them into the single big document during training and we also remove\\\\nduplicate words from test documents. Fig. 4.3 shows an example in which a...\",\"\\\\u0000it was pathetic the worst part was the\\\\nboxing scenes\\\\n\\\\u0000no plot twists or great scenes\\\\n+and satire and great plot twists\\\\n+great scenes great \\\\ufb01lm\\\\nAfter per-document binarization:\\\\n\\\\u0000it was pathetic the worst part boxing\\\\nscenes\\\\n\\\\u0000no plot twists or great scenes\\\\n+and satire great plot twists\\\\n+great scenes \\\\ufb01lmNB Binary\\\\nCounts Counts\\\\n+\\\\u0000+\\\\u0000\\\\nand 2 0 1 0\\\\nboxing 0 1 0 1\\\\n\\\\ufb01lm 1 0 1 0\\\\ngreat 3 1 2 1\\\\nit 0 1 0 1\\\\nno 0 1 0 1\\\\nor 0 1 0 1\\\\npart 0 1 0 1\\\\npathetic 0 1 0 1\\\\nplot 1 1 1 1\\\\nsatire 1 0 1 0\\\\nscenes 1 2 1 2\\\\nthe 0 2 0 ...\",\"didn\\\\u2019t completely alters the inferences we draw from the predicate like. Similarly,\\\\nnegation can modify a negative word to produce a positive review ( don\\\\u2019t dismiss this\\\\n\\\\ufb01lm,doesn\\\\u2019t let us get bored ).\\\\nA very simple baseline that is commonly used in sentiment analysis to deal with\\\\nnegation is the following: during text normalization, prepend the pre\\\\ufb01x NOT to\\\\nevery word after a token of logical negation ( n\\\\u2019t, not, no, never ) until the next punc-\\\\ntuation mark. Thus the phrase\\\\ndidn&#x27;t like this mo...\",\"these negation words and the predicates they modify, but this simple baseline works\\\\nquite well in practice.\\\\nFinally, in some situations we might have insuf\\\\ufb01cient labeled training data to\\\\ntrain accurate naive Bayes classi\\\\ufb01ers using all words in the training set to estimate\\\\npositive and negative sentiment. In such cases we can instead derive the positive\\\\nand negative word features from sentiment lexicons , lists of words that are pre-sentiment\\\\nlexicons\\\\nannotated with positive or negative sentiment...\",\"+:admirable, beautiful, con\\\\ufb01dent, dazzling, ecstatic, favor, glee, great\\\\n\\\\u0000:awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate\\\\nA common way to use lexicons in a naive Bayes classi\\\\ufb01er is to add a feature\\\\nthat is counted whenever a word from that lexicon occurs. Thus we might add a\\\\nfeature called \\\\u2018this word occurs in the positive lexicon\\\\u2019, and treat all instances of\\\\nwords in the lexicon as counts for that one feature, instead of counting each word\\\\nseparately. Similarly, we might...\",\"We\\\\u2019ll return to this use of lexicons in Chapter 22, showing how these lexicons\\\\ncan be learned automatically, and how they can be applied to many other tasks be-\\\\nyond sentiment classi\\\\ufb01cation.\\\\n4.5 Naive Bayes for other text classi\\\\ufb01cation tasks\\\\nIn the previous section we pointed out that naive Bayes doesn\\\\u2019t require that our\\\\nclassi\\\\ufb01er use all the words in the training data as features. In fact features in naive\\\\nBayes can express any property of the input text we want.\\\\nConsider the task of spam detec...\",\"that are not purely linguistic. For example the open-source SpamAssassin tool2\\\\nprede\\\\ufb01nes features like the phrase \\\\u201cone hundred percent guaranteed\\\\u201d, or the feature\\\\nmentions millions of dollars , which is a regular expression that matches suspiciously\\\\nlarge sums of money. But it also includes features like HTML has a low ratio of text\\\\nto image area , that aren\\\\u2019t purely linguistic and might require some sophisticated\\\\ncomputation, or totally non-linguistic features about, say, the path that the emai...\",\"For other tasks, like language id \\\\u2014determining what language a given piece language id\\\\nof text is written in\\\\u2014the most effective naive Bayes features are not words at all,\\\\nbutcharacter n-grams , 2-grams (\\\\u2018zw\\\\u2019) 3-grams (\\\\u2018nya\\\\u2019, \\\\u2018 V o\\\\u2019), or 4-grams (\\\\u2018ie z\\\\u2019,\\\\n\\\\u2018thei\\\\u2019), or, even simpler byte n-grams , where instead of using the multibyte Unicode\\\\ncharacter representations called codepoints, we just pretend everything is a string of\\\\nraw bytes. Because spaces count as a byte, byte n-grams can model statist...\",\"pedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire.\\\\nTo make sure that this multilingual text correctly re\\\\ufb02ects different regions, dialects,\\\\nand socioeconomic classes, systems also add Twitter text in many languages geo-\\\\ntagged to many regions (important for getting world English dialects from countries\\\\nwith large Anglophone populations like Nigeria or India), Bible and Quran transla-\\\\ntions, slang websites like Urban Dictionary, corpora of African American Vern...\",\"in the text (not a subset), then naive Bayes has an important similarity to language\\\\nmodeling. Speci\\\\ufb01cally, a naive Bayes model can be viewed as a set of class-speci\\\\ufb01c\\\\nunigram language models, in which the model for each class instantiates a unigram\\\\nlanguage model.\\\\nSince the likelihood features from the naive Bayes model assign a probability to\\\\neach word P(wordjc), the model also assigns a probability to each sentence:\\\\nP(sjc) =Y\\\\ni2positionsP(wijc) (4.15)\\\\nThus consider a naive Bayes model with th...\",\"P(\\\\u201cI love this fun \\\\ufb01lm\\\\u201d j\\\\u0000) = 0:2\\\\u00020:001\\\\u00020:01\\\\u00020:005\\\\u00020:1=1:0\\\\u000210\\\\u00009\\\\nAs it happens, the positive model assigns a higher probability to the sentence:\\\\nP(sjpos)&gt;P(sjneg). Note that this is just the likelihood part of the naive Bayes\\\\nmodel; once we multiply in the prior a full naive Bayes model might well make a\\\\ndifferent classi\\\\ufb01cation decision.\\\\n4.7 Evaluation: Precision, Recall, F-measure\\\\nTo introduce the methods for evaluating text classi\\\\ufb01cation, let\\\\u2019s \\\\ufb01rst consider some\\\\nsimple binary detection tasks. ...\",\"match. We will refer to these human labels as the gold labels . gold labels\\\\nOr imagine you\\\\u2019re the CEO of the Delicious Pie Company and you need to know\\\\nwhat people are saying about your pies on social media, so you build a system that\\\\ndetects tweets concerning Delicious Pie. Here the positive class is tweets about\\\\nDelicious Pie and the negative class is all other tweets.\\\\nIn both cases, we need a metric for knowing how well our spam detector (or\\\\npie-tweet-detector) is doing. To evaluate any syste...\",\"are documents that are indeed spam (indicated by human-created gold labels) that\\\\nour system correctly said were spam. False negatives are documents that are indeed\\\\nspam but our system incorrectly labeled as non-spam.\\\\nTo the bottom right of the table is the equation for accuracy , which asks what\\\\npercentage of all the observations (for the spam or pie examples that means all emails\\\\nor tweets) our system labeled correctly. Although accuracy might seem a natural\\\\nmetric, we generally don\\\\u2019t use it fo...\",\"while the other 999,900 are tweets about something completely unrelated. Imagine a\\\\nsimple classi\\\\ufb01er that stupidly classi\\\\ufb01ed every tweet as \\\\u201cnot about pie\\\\u201d. This classi\\\\ufb01er\\\\nwould have 999,900 true negatives and only 100 false negatives for an accuracy of\\\\n999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should\\\\nbe happy with this classi\\\\ufb01er? But of course this fabulous \\\\u2018no pie\\\\u2019 classi\\\\ufb01er would\\\\nbe completely useless, since it wouldn\\\\u2019t \\\\ufb01nd a single one of the customer comments\\\\nwe ...\",\"Figure 4.4 A confusion matrix for visualizing how well a binary classi\\\\ufb01cation system per-\\\\nforms against gold standard labels.\\\\nThat\\\\u2019s why instead of accuracy we generally turn to two other metrics shown in\\\\nFig. 4.4: precision andrecall .Precision measures the percentage of the items that precision\\\\nthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,\\\\nare positive according to the human gold labels). Precision is de\\\\ufb01ned as\\\\nPrecision =true positives\\\\ntrue positi...\",\"a terrible recall of 0 (since there are no true positives, and 100 false negatives, the\\\\nrecall is 0/100). You should convince yourself that the precision at \\\\ufb01nding relevant\\\\ntweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize\\\\ntrue positives: \\\\ufb01nding the things that we are supposed to be looking for.\\\\nThere are many ways to de\\\\ufb01ne a single metric that incorporates aspects of both\\\\nprecision and recall. The simplest of these combinations is the F-measure (van F-measure\\\\n...\",\"F1=2PR\\\\nP+R(4.16)\\\\nF-measure comes from a weighted harmonic mean of precision and recall. The\\\\nharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-\\\\nrocals:\\\\nHarmonicMean (a1;a2;a3;a4;:::;an) =n\\\\n1\\\\na1+1\\\\na2+1\\\\na3+:::+1\\\\nan(4.17)68 CHAPTER 4 \\\\u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\\\\nand hence F-measure is\\\\nF=1\\\\na1\\\\nP+(1\\\\u0000a)1\\\\nRor\\\\u0012\\\\nwithb2=1\\\\u0000a\\\\na\\\\u0013\\\\nF=(b2+1)PR\\\\nb2P+R(4.18)\\\\nHarmonic mean is used because the harmonic mean of two values is closer to the\\\\nminimum of the two value...\",\"For sentiment analysis we generally have 3 classes (positive, negative, neutral) and\\\\neven more classes are common for tasks like part-of-speech tagging, word sense\\\\ndisambiguation, semantic role labeling, emotion detection, and so on. Luckily the\\\\nnaive Bayes algorithm is already a multi-class classi\\\\ufb01cation algorithm.\\\\n851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+30200...\",\"example, that the system mistakenly labeled one spam document as urgent, and we\\\\nhave shown how to compute a distinct precision and recall value for each class. In\\\\norder to derive a single metric that tells us how well the system is doing, we can com-\\\\nbine these values in two ways. In macroaveraging , we compute the performance macroaveraging\\\\nfor each class, and then average over classes. In microaveraging , we collect the de- microaveraging\\\\ncisions for all classes into a single confusion matrix,...\",\"statistics of the smaller classes, and so is more appropriate when performance on all\\\\nthe classes is equally important.4.8 \\\\u2022 T EST SETS AND CROSS -VALIDATION 69\\\\n8811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=PooledClass 3: SpamC...\",\"thedevelopment test set (also called a devset ) to perhaps tune some parameters,development\\\\ntest set\\\\ndevset and in general decide what the best model is. Once we come up with what we think\\\\nis the best model, we run it on the (hitherto unseen) test set to report its performance.\\\\nWhile the use of a devset avoids over\\\\ufb01tting the test set, having a \\\\ufb01xed train-\\\\ning set, devset, and test set creates another problem: in order to save lots of data\\\\nfor training, the test set (or devset) might not be large...\",\"classi\\\\ufb01er on the remaining k\\\\u00001 folds, and then compute the error rate on the test\\\\nset. Then we repeat with another fold as the test set, again training on the other k\\\\u00001\\\\nfolds. We do this sampling process ktimes and average the test set error rate from\\\\nthese kruns to get an average error rate. If we choose k=10, we would train 10\\\\ndifferent models (each on 90% of our data), test the model 10 times, and average\\\\nthese 10 values. This is called 10-fold cross-validation .10-fold\\\\ncross-validation\\\\nThe o...\",\"on is important in designing NLP systems! What to do? For this reason, it is com-\\\\nmon to create a \\\\ufb01xed training set and test set, then do 10-fold cross-validation inside\\\\nthe training set, but compute error rate the normal way in the test set, as shown in\\\\nFig. 4.7.70 CHAPTER 4 \\\\u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\\\\nTraining Iterations13452678910DevDevDevDevDevDevDevDevDevDevTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTest SetTesting\\\\nFigure ...\",\"for NLP classi\\\\ufb01ers, drawing especially on the work of Dror et al. (2020) and Berg-\\\\nKirkpatrick et al. (2012).\\\\nSuppose we\\\\u2019re comparing the performance of classi\\\\ufb01ers AandBon a metric M\\\\nsuch as F 1, or accuracy. Perhaps we want to know if our logistic regression senti-\\\\nment classi\\\\ufb01er A(Chapter 5) gets a higher F 1score than our naive Bayes sentiment\\\\nclassi\\\\ufb01er Bon a particular test set x. Let\\\\u2019s call M(A;x)the score that system Agets\\\\non test set x, and d(x)the performance difference between AandBonx:...\",\"the F 1score of Ais higher than B\\\\u2019s by .04. Can we be certain that Ais better? We\\\\ncannot! That\\\\u2019s because Amight just be accidentally better than Bon this particular x.\\\\nWe need something more: we want to know if A\\\\u2019s superiority over Bis likely to hold\\\\nagain if we checked another test set x0, or under some other set of circumstances.\\\\nIn the paradigm of statistical hypothesis testing, we test this by formalizing two\\\\nhypotheses.\\\\nH0:d(x)\\\\u00140\\\\nH1:d(x)&gt;0 (4.20)\\\\nThe hypothesis H0, called the null hypothesi...\",\"we would encounter the value of d(x)that we found, if we repeated the experiment\\\\na great many times. We formalize this likelihood as the p-value : the probability, p-value\\\\nassuming the null hypothesis H0is true, of seeing the d(x)that we saw or one even\\\\ngreater\\\\nP(d(X)\\\\u0015d(x)jH0is true ) (4.21)\\\\nSo in our example, this p-value is the probability that we would see d(x)assuming\\\\nAisnotbetter than B. Ifd(x)is huge (let\\\\u2019s say Ahas a very respectable F 1of .9\\\\nandBhas a terrible F 1of only .2 on x), we mig...\",\"under the null hypothesis, and we can reject the null hypothesis. What counts as very\\\\nsmall? It is common to use values like .05 or .01 as the thresholds. A value of .01\\\\nmeans that if the p-value (the probability of observing the dwe saw assuming H0is\\\\ntrue) is less than .01, we reject the null hypothesis and assume that Ais indeed better\\\\nthan B. We say that a result (e.g., \\\\u201c Ais better than B\\\\u201d) is statistically signi\\\\ufb01cant ifstatistically\\\\nsigni\\\\ufb01cant\\\\nthedwe saw has a probability that is below the ...\",\"usually use non-parametric tests based on sampling: we arti\\\\ufb01cially create many ver-\\\\nsions of the experimental setup. For example, if we had lots of different test sets x0\\\\nwe could just measure all the d(x0)for all the x0. That gives us a distribution. Now\\\\nwe set a threshold (like .01) and if we see in this distribution that 99% or more of\\\\nthose deltas are smaller than the delta we observed, i.e., that p-value( x)\\\\u2014the proba-\\\\nbility of seeing a d(x)as big as the one we saw\\\\u2014is less than .01, then w...\",\"Paired tests are those in which we compare two sets of observations that are aligned: paired\\\\neach observation in one set can be paired with an observation in another. This hap-\\\\npens naturally when we are comparing the performance of two systems on the same\\\\ntest set; we can pair the performance of system Aon an individual observation xi\\\\nwith the performance of system Bon the same xi.\\\\n4.9.1 The Paired Bootstrap Test\\\\nThebootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from pre- ...\",\"edly sampling from it. The method only makes the assumption that the sample is\\\\nrepresentative of the population.72 CHAPTER 4 \\\\u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\\\\nConsider a tiny text classi\\\\ufb01cation example with a test set xof 10 documents. The\\\\n\\\\ufb01rst row of Fig. 4.8 shows the results of two classi\\\\ufb01ers (A and B) on this test set.\\\\nEach document is labeled by one of the four possibilities (A and B both right, both\\\\nwrong, A right and B wrong, A wrong and B right). A slash through a lette...\",\"n=10. Fig. 4.8 shows a couple of examples. To create each virtual test set x(i), we\\\\nrepeatedly ( n=10 times) select a cell from row xwith replacement. For example, to\\\\ncreate the \\\\ufb01rst cell of the \\\\ufb01rst virtual test set x(1), if we happened to randomly select\\\\nthe second cell of the xrow; we would copy the value A \\\\u0013B into our new cell, and\\\\nmove on to create the second cell of x(1), each time sampling (randomly choosing)\\\\nfrom the original xwith replacement.\\\\n1 2 3 4 5 6 7 8 910A% B% d()\\\\nx AB A\\\\u0013\\\\u0013BAB\\\\u0000\\\\u0000A...\",\"replacement; thus an individual sample is a single cell, a document with its gold label and\\\\nthe correct or incorrect performance of classi\\\\ufb01ers A and B. Of course real test sets don\\\\u2019t have\\\\nonly 10 examples, and bneeds to be large as well.\\\\nNow that we have the btest sets, providing a sampling distribution, we can do\\\\nstatistics on how often Ahas an accidental advantage. There are various ways to\\\\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\\\\net al. (2012). Assuming ...\",\"value by d(x)or more:\\\\np-value (x) =1\\\\nbbX\\\\ni=11\\\\u0010\\\\nd(x(i))\\\\u0000d(x)\\\\u00150\\\\u0011\\\\n(We use the notation 1(x)to mean \\\\u201c1 if xis true, and 0 otherwise\\\\u201d.) However,\\\\nalthough it\\\\u2019s generally true that the expected value of d(X)over many test sets,\\\\n(again assuming Aisn\\\\u2019t better than B) is 0, this isn\\\\u2019t true for the bootstrapped test\\\\nsets we created. That\\\\u2019s because we didn\\\\u2019t draw these samples from a distribution\\\\nwith 0 mean; we happened to create them from the original test set x, which happens\\\\nto be biased (by .20) in fav...\",\"of the test sets do we \\\\ufb01nd that A is accidentally better d(x(i))\\\\u00152d(x), the resulting\\\\np-value of .0047 is smaller than .01, indicating that the delta we found, d(x)is indeed\\\\nsuf\\\\ufb01ciently surprising and unlikely to have happened by accident, and we can reject\\\\nthe null hypothesis and conclude Ais better than B.\\\\nfunction BOOTSTRAP (test set x,num of samples b)returns p-value (x)\\\\nCalculate d(x)# how much better does algorithm A do than B on x\\\\ns= 0\\\\nfori= 1tobdo\\\\nforj= 1tondo # Draw a bootstrap sample x...\",\"The full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set x, a\\\\nnumber of samples b, and counts the percentage of the bbootstrap test sets in which\\\\nd(x\\\\u0003(i))&gt;2d(x). This percentage then acts as a one-sided empirical p-value.\\\\n4.10 Avoiding Harms in Classi\\\\ufb01cation\\\\nIt is important to avoid harms that may result from classi\\\\ufb01ers, harms that exist both\\\\nfor naive Bayes classi\\\\ufb01ers and for the other classi\\\\ufb01cation algorithms we introduce\\\\nin later chapters.\\\\nOne class of harms is repres...\"]],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\"],\"shape\":[150],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1045\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1046\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1041\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1042\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1012\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1028\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1029\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1030\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1031\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1037\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1022\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1023\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1024\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1025\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1017\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1018\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1019\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1020\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1021\",\"attributes\":{\"axis\":{\"id\":\"p1017\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1026\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1022\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1047\",\"attributes\":{\"title\":\"Knowledge Base Tospics\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1048\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Language Modeling\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":17}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1049\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"String Algorithms\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":0}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}},{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1119\",\"attributes\":{\"title\":\"Failures\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1055\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1064\",\"attributes\":{\"start\":5.435843300819397,\"end\":17.402650928497316}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1065\",\"attributes\":{\"start\":-11.130775928497314,\"end\":0.03082120418548584}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1066\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1067\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1062\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1094\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1051\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1052\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1053\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"K88EQcKAAUF60x9BOoslQQyuCEE=\"},\"shape\":[5],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"cWCUwFyTBMGz+njAFxPqwPecNMA=\"},\"shape\":[5],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\",\"Language Modeling\"]],[\"correctness\",[true,true,false,true,true]],[\"questions\",[\"What is the solution for unknown words that did not appear in the training documents?\",\"Explain the Markov assumption in the context of n-gram language models.  Specifically, how does this assumption simplify the calculation of word probabilities, and what are the trade-offs involved in using this approximation for bigrams and n-grams (N&gt;2)?\",\"Considering the context is about evaluating language models using perplexity, and not about spam detection,  what features, if any, could be conceptually analogous to SpamAssassin&#x27;s spam detection features,  when applied to the task of identifying low-quality or nonsensical text sequences based on the provided text&#x27;s discussion of perplexity and language models?\",\"Hi, I&#x27;m a graduate student working on my thesis, and I&#x27;m using add-one smoothing for my bigram counts. I&#x27;m a bit frustrated because it&#x27;s drastically changing my word counts. For example, the count for the bigram \\\\\"want to\\\\\" went from 608 to 238.  Could you tell me what the effect of add-one smoothing was on the count of that specific bigram?\",\"What is the task of text categorization, and what is sentiment analysis?\"]],[\"agent_answer\",[\"The solution for unknown words (unknown word) is to ignore them\\\\u2014remove them from the test document and not include any probability for them at all.\\\\n\",\"The Markov assumption in n-gram language models states that the probability of a word depends only on the preceding *n-1* words.  For a bigram model, this means the probability of a word depends only on the immediately preceding word.  For a trigram model, it depends on the two preceding words, and so on.\\\\n\\\\nThis assumption simplifies probability calculations by allowing us to approximate the true probability of a word given its entire history (P(wn|w1:n-1)) with the probability given only the *n-1* preceding words (P(wn|wn-N+1:n-1)).  Instead of considering the entire context, we only look at a limited window of previous words.\\\\n\\\\nThe trade-offs are as follows:\\\\n\\\\n* **Computational Efficiency:**  The Markov assumption drastically reduces the computational cost.  Calculating the true probability considering all preceding words would be extremely complex and require vast amounts of data.  The approximation makes the computation feasible.\\\\n\\\\n* **Loss of Information:** By limiting the context, we lose potentially valuable information.  Long-range dependencies between words outside the n-gram window are ignored.  For example, in the sentence \\\\\"The cat sat on the mat,\\\\\" a bigram model would capture the relationship between \\\\\"the\\\\\" and \\\\\"cat,\\\\\" and \\\\\"cat\\\\\" and \\\\\"sat,\\\\\" but not the relationship between \\\\\"cat\\\\\" and \\\\\"mat,\\\\\" which are semantically linked.  Larger n-grams (trigrams, 4-grams, etc.) capture more context and improve accuracy but increase computational complexity.\\\\n\",\"Answer Not Available In Given Context\\\\n\",\"The add-one smoothing reduced the count of the bigram \\\\\"want to\\\\\" from 608 to 238.  This represents a discount factor of 0.39 (238/608).\\\\n\",\"Text categorization is the task of assigning a label or category to an entire text or document. Sentiment analysis is one common text categorization task; it is the extraction of sentiment, the positive or negative orientation that a writer expresses toward some object.  For example, a review of a movie or product expresses the author&#x27;s sentiment, and an editorial expresses sentiment toward a political action or candidate.\\\\n\"]],[\"reference_answer\",[\"The solution for such unknown words is to ignore them\\\\u2014remove them from the test unknown word document and not include any probability for them at all.\",\"The Markov assumption states that the probability of a word depends only on the preceding word.  N-gram models (like bigrams and trigrams) utilize this assumption by approximating the probability of a word given its entire history with the probability given only the last few words.  For example, a bigram model uses only the preceding word, while a trigram uses the two preceding words.\",\"SpamAssassin uses features like the phrase \\\\\"one hundred percent guaranteed\\\\\", a regular expression matching suspiciously large sums of money, a low ratio of text to image area in HTML, unbalanced \\\\\"head\\\\\" tags in HTML, and the presence of phrases of urgency like \\\\\"urgent reply\\\\\".  It also considers the email subject line being all capital letters, the subject line containing \\\\\"online pharmaceutical\\\\\", and claims of removal from a mailing list.  Additionally, it may use non-linguistic features related to the email&#x27;s path.\",\"Add-one smoothing changed the count of &#x27;want to&#x27; from 608 to 238.\",\"Text categorization is the task of assigning a label or category to an entire text or document. Sentiment analysis is the extraction of sentiment, the positive or negative orientation that a writer expresses toward some object.\"]],[\"id\",[211,124,221,169,194]],[\"content\",[\"ulary at all because they did not occur in any training document in any class? The\\\\nsolution for such unknown words is to ignore them\\\\u2014remove them from the test unknown word\\\\ndocument and not include any probability for them at all.\\\\nFinally, some systems choose to completely ignore another class of words: stop\\\\nwords , very frequent words like theanda. This can be done by sorting the vocabu- stop words\\\\nlary by frequency in the training set, and de\\\\ufb01ning the top 10\\\\u2013100 vocabulary entries\\\\nas stop words...\",\"exact probability of a word given a long sequence of preceding words, P(wnjw1:n\\\\u00001).\\\\nAs we said above, we can\\\\u2019t just estimate by counting the number of times every word\\\\noccurs following every long string in some corpus, because language is creative and\\\\nany particular context might have never occurred before!\\\\n3.1.1 The Markov assumption\\\\nThe intuition of the n-gram model is that instead of computing the probability of a\\\\nword given its entire history, we can approximate the history by just the last ...\",\"that are not purely linguistic. For example the open-source SpamAssassin tool2\\\\nprede\\\\ufb01nes features like the phrase \\\\u201cone hundred percent guaranteed\\\\u201d, or the feature\\\\nmentions millions of dollars , which is a regular expression that matches suspiciously\\\\nlarge sums of money. But it also includes features like HTML has a low ratio of text\\\\nto image area , that aren\\\\u2019t purely linguistic and might require some sophisticated\\\\ncomputation, or totally non-linguistic features about, say, the path that the emai...\",\"c\\\\u0003(wn\\\\u00001wn) =[C(wn\\\\u00001wn)+1]\\\\u0002C(wn\\\\u00001)\\\\nC(wn\\\\u00001)+V(3.28)\\\\nNote that add-one smoothing has made a very big change to the counts. Com-\\\\nparing Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to )changed\\\\nfrom 608 to 238! We can see this in probability space as well: P(tojwant)decreases\\\\nfrom 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the dis-\\\\ncount d(the ratio between new and old counts) shows us how strikingly the counts\\\\nfor each pre\\\\ufb01x word have been reduced; th...\",\"tremble as if they were mad, (j) innumerable ones, (k) those drawn with\\\\na very \\\\ufb01ne camel\\\\u2019s hair brush, (l) others, (m) those that have just broken\\\\na \\\\ufb02ower vase, (n) those that resemble \\\\ufb02ies from a distance.\\\\nMany language processing tasks involve classi\\\\ufb01cation, although luckily our classes\\\\nare much easier to de\\\\ufb01ne than those of Borges. In this chapter we introduce the naive\\\\nBayes algorithm and apply it to text categorization , the task of assigning a label ortext\\\\ncategorization\\\\ncategory to an ent...\"]],[\"color\",[\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1095\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1096\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1091\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1092\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1093\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1109\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1100\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1101\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1102\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"W3dEQeRxPUF5TEtBvDZHQT7RUEFsPkpBWblOQVyyTkHe+URBzJ9EQbjXSUGhW0FB7uE3QQwePUHMqjVBj1Y0QRAMOkFWrBhBKEYUQTPKE0Gkzw1B8KoMQRhfC0HqnwJBwoABQZLkA0EtvAhBe5oTQU1rAkGXHwxBBD8kQUurHEGnCRxBGvISQfqy/kCtcA5BhmYHQas9AEE2ud9Ap4PhQORV5kD5Uu1A/p7TQKid0kCxUuFAQGnZQAjJ0UB3YepAZsn0QMsA2ECjXOtAJgb3QIBJ8UBM0wNBaTUNQRs+CUGM8QdB0DsOQSRQB0FBAA1B9WAMQYdODEHnyQNB/JEQQV1THEGMuhpB+0knQUx+JUFi7CJBOoslQcXWFEFuRBtBwLYVQRi9DUG0fRRBZZrfQDTw3kDRV+dAC//bQO2a80AfLPxAGI/oQEXZ4kDv69lAfAcCQdXZG0FMYRxB7S0fQZcrIUEHuRdBt0UfQf9VGkFWKCJBXiQCQQyuCEGAXQ1BOvYdQYiTDUFuKAJBKUH7QGTYEkE35A9By78KQfcwCEF5gxRBOsgIQe9KE0Hm4glBj54RQS1iC0E1YBZBK88EQQVACEEBlApBBcsDQZUtFEGozhFBu3oQQYFJGkEoKhZBVq4ZQXrTH0GPlR5BCU0bQRwHDkEW1wFBHH0DQT77AEEtXvJAUkj7QIPj6kCDd/VAwCwBQYqQ8UBLkOdAhlfaQI3d6UDyud1Aa6jZQE1qw0AgC8hAhS/HQKGwyEAd+8ZAq2L1QGmGxkBceNNA5lzSQBmNuUC6m9tA\"},\"shape\":[150],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"xyDRwEV1wMDoHNHA26XGwCKyyMAxMrTAc/a4wEmptMC2hr7AdNDMwKgHvcB4tLPAYRu7wH+fqcDyYcbAx23XwNTf2sCsyfzATzT0wISe/cATSPHADsELwaxh7MCeO+zAXJMEwYHOBcFfkQnBvP8HwRIKEMECVBTBFSkNwZ/+EsECQgzBfogOwVZj+MAplADBio3+wGXwtcBVH7bAyZGfwDwrqcD2GqXAXAi4wDNfqMAmydDAeNTKwCLD1MC+2drAjqfXwFZDyMACx8bAJMzNwKibw8BsgcvAgeruwMKa9cCR4ADB2egEwScn3MBZS97AgBOswNurwcB+MrHAvQC4wKE3ucD/mNXAnFwGwfU4AMHSmhHBFxPqwDAR0cCem+LAPpvjwDNDysDb8MDAXA7hwM1888AIVgLBV4QAwfWE68AwkALBbt3ywLNQ8sCwWOXAKWvSwCmP0MBtyeXAPmPLwPypw8CU3f7AaDMOwfdABcFVKfnAJbt+wPecNMC92A7AEv1lwJiEa8CzlWPAAGeCwMIFf8BObV/ARRc7wP+KiMBPDo/A/yaawC20f8BbknnAbDSFwC6qksA5nqDAcWCUwDa0VcAdiojACYxgwCyrOcBW1QzASwsmwMyEQMD0IzTAhaBUwLP6eMDq/4nAA2iOwH/WUsAC0B/ACXr8v1MEDMCk7gnAQSvqvyI/EMCkOzTAKb8uwKn6J8BEL2TAEYeOwIJdhcC7UHXAF5Y/wMeYYcD+TmjA30ODwOg+bMBrZ1HAhH9TwDxpK8B1i0jATUc0wGUCRsCC7yrA\"},\"shape\":[150],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1110\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1111\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1106\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1107\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1108\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1063\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1078\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1079\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1080\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1081\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1086\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1087\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1054\",\"attributes\":{\"renderers\":[{\"id\":\"p1094\"}],\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1073\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1074\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1075\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1076\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1068\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1069\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1070\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1071\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1072\",\"attributes\":{\"axis\":{\"id\":\"p1068\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1077\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1073\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1097\",\"attributes\":{\"title\":\"Question Correctness\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1098\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"False\"},\"renderers\":[{\"id\":\"p1094\"}],\"index\":2}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1099\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"True\"},\"renderers\":[{\"id\":\"p1094\"}],\"index\":0}}]}},{\"type\":\"object\",\"name\":\"LabelSet\",\"id\":\"p1115\",\"attributes\":{\"level\":\"glyph\",\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1112\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1113\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1114\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"2HRDQXB5BUE=\"},\"shape\":[2],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"+KrCwEbMssA=\"},\"shape\":[2],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"String Algorithms\",\"Language Modeling\"]]]}}},\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"topic\"},\"text_color\":{\"type\":\"value\",\"value\":\"#B1B1B1\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"12pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}}],\"tabs_location\":\"below\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"651aa3fc-9bb5-422c-a02f-5f6851b6f043\",\"roots\":{\"p1120\":\"b8591bb2-6beb-443d-9e2a-89318e1c9579\"},\"root_ids\":[\"p1120\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"b8591bb2-6beb-443d-9e2a-89318e1c9579\" data-root-id=\"p1120\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "\n",
       "                <div class=\"section-title\">SELECTED METRICS</div>\n",
       "\n",
       "                \n",
       "\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "    function opentab(evt, name) {\n",
       "    // Declare all variables\n",
       "    let i, tabcontent, tablinks;\n",
       "\n",
       "    // Get all elements with class=\"tabcontent\" and hide them\n",
       "    tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
       "    for (i = 0; i < tabcontent.length; i++) {\n",
       "        tabcontent[i].style.display = \"none\";\n",
       "    }\n",
       "\n",
       "    // Get all elements with class=\"tablinks\" and remove the class \"active\"\n",
       "    tablinks = document.getElementsByClassName(\"tablinks\");\n",
       "    for (i = 0; i < tablinks.length; i++) {\n",
       "        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
       "    }\n",
       "\n",
       "    // Show the current tab, and add an \"active\" class to the button that opened the tab\n",
       "    document.getElementById(name).style.display = \"block\";\n",
       "    evt.currentTarget.className += \" active\";\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<giskard.rag.report.RAGReport at 0x13f9c1950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
